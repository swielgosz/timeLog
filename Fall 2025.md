# December 3
Recap:
We are currently working on investigating the effects of having fixed the mlp output. We suspect that if we have a corrected output, we will improve generalization abilities. Yesterday, we tested if the new pipeline improved generalization ability of the model. We trained on 100 random complex orbits, and tested on a separate validation set of 100 random complex orbits. We first tested this by comparing results of training using the conference hifi config vsThe new pipeline did slightly improve generalization, and notably the lofi config was as good or better than the hifi config. I think this is mainly because lofi config switched to leaky relu. Let's see if this is true - if we change just the activation function, what happens?

TODO:
- [ ] plot datasets
- [ ] train with new and old pipeline on 1 and 10 complex orbits to compare ability to generalize
- [ ] try lofi with new and old pipeline with tanh vs leaky relu
- [ ] investigate difference in convergence times
- [ ] implement at least one visualization that John will like
- [ ] can segment length make dynamics linear? how can we test this?
- [ ] what is the effect of activation on output layers?
- [ ] how will different input/output layers affect training?
- [ ] add flag for cpu vs gpu
- [ ] figure out how to access runtime for wandb
- [ ] box and whisker plot visualization?
- [ ] brainstorm visualizations/sensitivities we want to run
- [ ] try segmentation length sweep
- [ ] implement basic latent ODE?
- [ ] after hours - clean up commit history


knobs to turn:
1. leaky_relu vs tanh
2. batch size
3. output layer and activation
## comparing lofi results
We want to see if fixing the output layer allows us to use a less handholdy training method. To test this, let's run the following tests:
1. Use the same parameters as the conference hifi config *except* do less steps and no length strategy. Use incorrect output
2. Use the same parameters as the conference hifi config *except* do less steps, no length strategy, and leaky_relu instead of tanh. Use incorrect output
3. Use the same parameters as the conference hifi config *except* do less steps and no length strategy. Use correct output
4. Use the same parameters as the conference hifi config *except* do less steps, no length strategy, and leaky_relu instead of tanh. Use correct output

### case 1
run_id: 76h805fh
runtime: 1:12
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-tanh"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
### case 2
run_id: bxlii4iv
runtime: 2:26
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-leaky_relu"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
why does leaky_relu take longer to train? this may just be a computer/wandb related thing/ why is my cpu usage 328%?
### case 3
run_id: rvkedxfy
runtime: 1:21
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-tanh"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

### case 4
run_id: eb38i1t2
runtime:
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

What we are trying to determine here is 1. is leaky relu significantly better than tanh, 2 is abs val output significantly better than signed output and 3 are these related

Metric: acceleration eror of model applied to test dataset

| label                                    | wandb_group_id                           | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :--------------------------------------- | :--------------------------------------- | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed-pe-tanh       | complex-lofi-mlp_4D_signed-pe-tanh       |                      11.7 |                       15.6 |                        11.8 |
| complex-lofi-mlp_4D_signed-pe-leaky_relu | complex-lofi-mlp_4D_signed-pe-leaky_relu |                      2.34 |                       2.11 |                        1.98 |
| complex-lofi-mlp_4D-pe-tanh              | complex-lofi-mlp_4D-pe-tanh              |                        19 |                       20.1 |                        16.3 |
| complex-lofi-mlp_4D-pe-leaky_relu        | complex-lofi-mlp_4D-pe-leaky_relu        |                      2.18 |                       2.72 |                        2.18 |

**Conclusions:**
- leaky_relu is sooo much better!! why is this? Let's poke at this a little bit but not dwell on it too much for now. 
- signed vs unsigned acceleration in the output layer doesn't have a major output on the overall results IN THIS CASE - the model is much more obviously improved by using leaky relu instead of tanh. I'm curious if it has an effect on convergence rate or output tracking though. Lets plot the losses and whatnot for the cases above. Also, we need to see if the results are different for when we train on fewer orbits! Let's repeat the test from yesterday for the other amounts of orbits

It doesn't look like there is a significant difference in convergence/noisiness using mlp_4D vs mlp_4D_signed

For Neural ODEs, smoothness and well-behaved derivatives often matter more than in standard nets because the ODE solver tracks the vector field continuously.

- Prefer smooth, non-saturating activations: Swish/SiLU, GELU, Softplus, ELU. They keep gradients flowing yet give a smooth vector field, which can reduce solver jitter and step count.
- ReLU/leaky ReLU work but introduce kinks; they’re usually fine, but very stiff dynamics can force smaller solver steps. If you notice step explosion or instability, try Softplus or Swish.
- Avoid hard saturation (tanh/sigmoid) unless you really need bounded dynamics; they can make the vector field flat and slow training.
- If you like leaky ReLU but want smoother negatives, PReLU (learned slope) or Softplus- shifted (e.g., softplus(x) - ln(2)) are easy swaps.
- Keep the final layer linear/identity so outputs can be negative; the hidden activation choice doesn’t prevent that.

## training on 1 complex orbit
### Abs value acceleration output
run_id: 2o6ir5kx
runtime: 0:48
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu-1"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_1_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
### Signed acceleration output
run_id: zclmkvzm
runtime: 0:49
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed_-pe-leaky_relu-1"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_1_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

| label                                       | wandb_group_id                              | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------------------------------------ | :------------------------------------------ | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed_-pe-leaky_relu-1 | complex-lofi-mlp_4D_signed_-pe-leaky_relu-1 |                       517 |                        433 |                         360 |
| complex-lofi-mlp_4D-pe-leaky_relu-1         | complex-lofi-mlp_4D-pe-leaky_relu-1         |                       459 |                        367 |                         299 |
## training on 10 complex orbits
### Abs value acceleration output
run_id: bhehtoo2
runtime: 1:54
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

### Signed acceleration output
run_id: rspv9fjn
runtime: 2:10
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

| label                                       | wandb_group_id                              | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------------------------------------ | :------------------------------------------ | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed-pe-leaky_relu-10 | complex-lofi-mlp_4D_signed-pe-leaky_relu-10 |                      4.61 |                       5.93 |                        5.38 |
| complex-lofi-mlp_4D-pe-leaky_relu-10        | complex-lofi-mlp_4D-pe-leaky_relu-10        |                      14.7 |                       10.6 |                        8.16 |
Hmm this isn't really what I expected to see? I wonder if this is because we are using a longer segment length again (length = 18/360 = 5% of orbit) - we don't see the flip flopping in this case. However, if we have a shorter segment length then the output layer may be more important. 
## John Report:

## Follow up
- When we apply the model to validation orbits, should we be applying to an initial condition, propagating for one whole orbit and then calculating acceleration error based on this? Or should we be segmenting the validation orbits in some way as well?
- Why is leaky relu so much better than tanh?
- how do I prevent my hands from freezing off
- does length strat matter?
- does segmentation strat matter?

## Notes to self:
- Make sure that if/when we put final activation back in, if we plot the output features then it should account for them going through this activation function. 

# December 2
## Datasets
### 1 simple
![[Pasted image 20251202121721.png|500]]
![[Pasted image 20251202121726.png]]

### 1 complex
![[Pasted image 20251202121714.png|500]]
![[Pasted image 20251202121738.png|1000]]
### 5 simple


### 5 complex
### 10 simple
### 10 complex
### 100 simple

### 100 complex
![[Pasted image 20251202121226.png]]
![[Pasted image 20251202121232.png]]

## Training
### complex-hifi-mlp_4D_signed
run_id: bl3987og (name different-aardvark-5734)
runtime: 4:42
config:
``` python
wandb:
  group: "complex-hifi-mlp_4D_signed"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:



  # EXHAUSTIVE LENGTH STRATEGY
  length_strategy:    [[
                        [0.0, 0.1],
                        [0.0, 0.2],
                        [0.0, 0.3],
                        [0.0, 0.4],
                        [0.0, 0.5],
                        [0.0, 0.6],
                        [0.0, 0.7],
                        [0.0, 0.8],
                        [0.0, 0.9],
                        [0.0, 1.0],]
                      ]

  lr_strategy: [[0.001, 0.001, 0.001,0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]]
  steps_strategy: [[500, 500, 500, 500, 500, 500, 500, 500, 500, 500]]
  segment_length_strategy: [[18,]]


  width: 64
  depth: 2
  train_val_split: 1
  batch_size: 64
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D_signed
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

## complex-hifi-mlp_4D
run_id: wkifs0zj (name denim-wave-5735)
runtime: 4:42

## complex-lofi-mlp_4D
run_id: kz4x0xl0 (name rose-resonance-5763)
runtime: 1:13

## complex-lofi-mlp_4D_peplusrmse
run_id: w6uiz2b4 (name vital-mountain-5737)
runtime: 1:25
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-peplusrmse"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 64
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error"
  loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

| label              | wandb_group_id                 | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |     |
| :----------------- | :----------------------------- | ------------------------: | -------------------------: | --------------------------: | --- |
| HiFi MLP (pe)      | complex-hifi-mlp_4D            |                      5.49 |                       5.48 |                        4.71 |     |
| LoFi MLP (pe)      | complex-lofi-mlp_4D            |                      2.62 |                       3.28 |                        2.73 |     |
| LoFi MLP (pe+rmse) | complex-lofi-mlp_4D-peplusrmse |                      2.76 |                       2.72 |                        2.34 |     |

Metric: average acceleration error
Testing datasets in last three columns

| training config label                | Acceleration output - signed (bad) or abs val (good) | Total steps | Runtime (min:sec) | Loss                                                | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :----------------------------------- | :--------------------------------------------------- | :---------- | :---------------- | :-------------------------------------------------- | ------------------------: | -------------------------: | --------------------------: |
| HiFi (conference config)             | signed                                               | 9500        | 10:02             | Percent error                                       |                      3.87 |                       3.57 |                        3.45 |
| HiFi (abs value acceleration output) | abs val                                              | 9500        | 10:21             | Percent error                                       |                      2.86 |                       3.08 |                        2.86 |
| LoFi                                 | signed                                               | 1000        | 1:08              | Percent error                                       |                      3.62 |                       3.30 |                        3.28 |
| LoFi                                 | signed                                               | 1000        | 1:15              | Percent error +  RMSE (normalized to similar scale) |                      2.56 |                       3.26 |                        3.60 |
| LoFi                                 | abs val                                              | 1000        | 1:18              | Percent error                                       |                      2.62 |                       3.28 |                        2.73 |
| LoFi                                 | abs val                                              | 1000        | 1:25              | Percent error +  RMSE (normalized to similar scale) |                      2.76 |                       2.72 |                        2.34 |



# November 26
Fixing commit history
# November 25:
- try fixing the train/val split back so that we are segmenting and then doing train/test split
- pipeline - length and segmentation strategy
- model - 
- training and testing on the same data is not good
	- perhaps we have explicit training and validation datasets
	- validation may be better
	- try different order of ops for split then segment and segment then split
- order of ops - exact same metrics, same pipeline
	- one orbit
	- we may have poor metrics
- set up code for additional metrics with validation sets
## training w longer segments
Differences:
- use percent error loss rather than percent error + rmse to make the model lighter
- segment orboits into 36 segments rather than 90
run_id: h8h0htoa
runtime: 1:09
config:
``` python
parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[2000]]
  segment_length_strategy: [[10,]]

  width: 32
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error_with_attraction"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  # output_layer: mlp_4D_activation
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple_hybrid
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```
![[Pasted image 20251125114415.png]]
# November 21
TODO:
- [ ] cleanish commit history
- [ ] document changes
- [ ] compare convergence for model a) before any chagnes were made, b) assuring force is attractive, c) 

# November 18
Breakthrough!! We realized that the feature layer that we were using, mlp_4D (and its variants), 
## baseline
``` python
wandb:
  group: "2BP-sensitivity"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[3000]]
  segment_length_strategy: [[4,]]

  width: 32
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

run_id: z0v2rmhl
time to run: 1:29


![[Pasted image 20251118115157.png]]
![[Pasted image 20251118115124.png]]
![[Pasted image 20251118115208.png]]
![[Pasted image 20251118115228.png]]
![[Pasted image 20251118115234.png]]
![[Pasted image 20251118115254.png]]


## calculating mlp_4D using absolute value of acceleration 
# November 18
- Spent last week mostly making QoL changes to codebase, but still need to figure out what's going on with training
- We frequently see that when we change the method of training, the model will lag (or lead) the true dynamics

Debugging this behavior:
![[Pasted image 20251112102231.png]]
![[Pasted image 20251117222526.png]]
Why is acceleration magnitude differnt between the two above?

![[Pasted image 20251117222555.png]]
![[Pasted image 20251117222606.png]]
![[Pasted image 20251117222617.png]]

``` python 
self.mlp = eqx.nn.MLP(
            in_size=in_size,
            out_size=out_size,
            width_size=width,
            depth=depth,
            activation=getattr(jnn, config.parameters.activation),
            # final_activation=jnn.tanh,
            key=key,
        )
        ```

The model doesn't look converged - what if we train for longer?
## Enforcing unit vector for r
``` python
data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 4
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  # output_layer: mlp_4D
  output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
![[Pasted image 20251117230413.png]]
![[Pasted image 20251117230421.png]]
![[Pasted image 20251117230431.png]]
![[Pasted image 20251117230354.png]]
![[Pasted image 20251117230458.png]]
![[Pasted image 20251117230448.png]]
Thoughts/questions:
- How does activation function affect things?
- final layer activation function necessary? 
- 


















# November 11

Goal: figure out why acceleration is dipping:
![[Pasted image 20251112102231.png]]
TODO:
- [ ] verify input and output features
- [ ] speed up training with features
- [ ] fix display of input and output features - why is it only happening for one orbit?
- [ ] clean up and commit code
- [ ] fix loss curves - training steps are not currently being plotted with the correct x axis

Why do we have __call__ and solve_with_feature_capture in NeuralODE class when they do basically the same thing? 
## Baseline
config:
``` python
parameters:

  length_strategy:
                      [[ 
                        [0.0, 1.0],                        
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 4
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  loss_fcn: "percent_error"

  activation: leaky_relu

  feature_layer: sph_4D_rinv_vel

  output_layer: mlp_4D

  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```
7j3cjn5u

## Activation functions:
![[Pasted image 20251112102520.png]]
# November 10

What does the output layer do?
Order of operations:
$y = [x, y, z, v_x, v_y, v_z]$ stores position and velocity. We load in our training data and normalize it, so y is already normalized before being passed into the training loop.
Then, the ODE we're learning needs $dy/dt$ which contains both $dx/dt$ (the velocities) and $dv/dt$ (the accelerations). The feature layer transforms $y$ into a compact/intelligent representation of our choice; the MLP turns that into a latent vector; the output-layer function turns that latent vector (and possibly the original stat) into a 6-vector $[\dot{x}, \dot{y}, \dot{z}, \dot{v_x}, \dot{v_y}, \dot{v_z}\}$. Diffrax integrats that to then compute losses and gradients.

**The end goal is always a 6‑dimensional dy/dt, but you don’t have to predict it all in one shot. Each output-layer helper defines a structured way to turn the MLP’s latent vector (and sometimes the current state) into that same derivative:

- mlp_simple assumes the MLP already emits a 6‑vector, so the head just returns it.
- mlp_simple_hybrid says “the first three components (dx/dt) should just be the current velocity,” so the MLP only predicts the acceleration part and we concatenate [vx, vy, vz, ax, ay, az].
- mlp_4D variants force the MLP to output an acceleration magnitude plus a direction vector, and the head reconstructs [vx, vy, vz, ax, ay, az] by combining the state’s velocities with that structured acceleration.

All of them end up with the same final derivative, but they impose different inductive biases or parameterizations on how the accelerations are produced. Choosing one head over another just changes how much structure you bake into the model before handing dy/dt to Diffrax and the losses.

**

xactly. mlp_output is just the latent vector coming out of the shared MLP—it’s the head’s job to interpret that latent into the actual state derivative. Depending on which output-layer helper you choose, that latent might encode:

- the entire 6‑vector derivative (mlp_simple),
- only the acceleration part (then the head splices in velocities from the state), or
- something more structured like [acc_magnitude, direction_vector] that the head expands back into accelerations.

So mlp_output itself isn’t the physical state; it’s whatever intermediate representation you’ve designed the head to work with before producing the final [dx/dt, dv/dt].
- Data prep happens in scripts/training/train.py: raw orbits are normalized (Normalization2BP), split, and batched. Each training batch supplies time grids ti, initial states yi[:,0,:], and masks.
    
- The model (neuralODE/neuralODE.py (lines 44-205)) is an Equinox module: a feature layer transforms each state y into a feature vector, a shared MLP maps features to an output vector, and an output-layer head (e.g., mlp_4D in neuralODE/output_layers.py (lines 12-66)) turns that vector into the 6‑dimensional state derivative. This derivative is the RHS fed to Diffrax’s diffeqsolve.
    
- Forward pass: for each trajectory in a batch, jax.vmap(model, in_axes=(0,0)) (neuralODE/losses.py (lines 6-22)) integrates the ODE across the time grid, producing predicted trajectories y_pred.
    
- Losses (neuralODE/losses.py (lines 23-209)) compare y_pred to the ground truth yi, typically via percent error or MSE variants. These losses may also add regularizers (L1/L2) or physics terms (energy drift).
    
- Backprop: Optax optimizers take loss(model, batch); JAX automatically differentiates through the Diffrax solver (Tsit5 with adjoint handled by Diffrax). Gradients flow from loss → predictions → solver states → model outputs → MLP weights/features. The optimizer updates the MLP parameters (and any scalars) each step.
    
- Metrics/visualizers: after training, helper routines capture feature-layer values, output-layer intermediates, and accelerations via solve_with_feature_capture, plot them (neuralODE/visualizers/feature_inputs.py), and log to WandB for diagnostics.

- [x] total loss, percent error, rms on the same plot
	- [ ] hope that we get tradeoff between loss functions - if not this may indicate that we are in a local minimum
	- [x] replace staircase representation in val loss with interpolated val loss
	- [x] simplify problem
	- [ ] John suspects activation function will help a lot - it looks like there is a critical radius where we go from being attracted to flying away. why do we go from attractor to repulsive force. we expect to see conical section still. 
	- [ ] accelerations should be negative in the radial direction - we could design to say a certain number must be negative. dot product between accel and position vector should be more than 90 degrees out of phase. show the direction of the acceleration vector. 

order of ops:
- [x]  simplify orbits
- change activation gelu, relu, leaky relu
- plot outputs for actual acceleration mag and direction
	- maybe new loss constraint - penalize if force isn't attractive
- expect to find a loss function that is suitable for different problems
- shouldn't really need to weight mse since everythign is already nondimensionalized

look at inputs as a function of time
John is concerned that position is constrained -1 to 1
plotting states as a function of time - true vs discrepant orbit 
input - velocity magnitude and an angle for that. or replace the velocity components completely.

QoL improvement - I want loss curves to save by default after training. This isn't happening right now . FIXED

I want to observe if acceleration is force is attractive and penalize otherwise. 
# November 7
Let's get a baseline without any learning curriculums for troubleshooting. `complex_TBP_planar_1`:
![[Pasted image 20251107160541.png|500]]
![[Pasted image 20251107160547.png|1000]]
complex_TBP_planar_4:
![[Pasted image 20251107161440.png|500]]
![[Pasted image 20251107161434.png|1000]]

## v1 - baseline
We'll start by training on complex_TBP_planar_4 to begin, with the following validation orbit:
![[Pasted image 20251107161725.png]]

The baseline config does not include any training curriculum:
``` python
data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],                        
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001```
losses:


![[Pasted image 20251107165931.png]]

![[Pasted image 20251107170022.png]]
# November 5

Debugging heat map.

Let's recall our baseline models that we are using for comparison and building from.
currently, we are testing on `complex_TBP_planar` which has 100 orbits. This might be too much for now, so let's reduce to 1, 4, 16 orbits (this is where our original heatmap issue came from anyway)

Recall these are the datasets we are using:
![[Pasted image 20251107095101.png|500]]
![[Pasted image 20251107095245.png|1000]]

Let's start by training on the dataset with 16 orbits. 
## v1 - segmentation strategy, no length strat, percent errror loss

config:
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 1.111%, 5%, 6.666% 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
losses:
![[Pasted image 20251107102932.png]]
![[Pasted image 20251107102946.png]]

randomly selected reference orbit:
![[Pasted image 20251107104833.png]]
![[Pasted image 20251107104900.png]]

- As we increase our segment lengh, the acceleration error does initially improve but then spikes. This could indicate that the segment length in phase 3 is too long, or we have a earning rate is too high because we see that our loss and acceleration error just oscillate
- Before we declare that the length is too long, let's try lowering the learning rate for the final phase. We should probably implement a learning rate decay schedule

## v2 - segmentation strategy, no length strat, percent error loss, lower lr for final phase

Difference from previous - lower the learning rate in the last phase
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001```

loss:
![[Pasted image 20251107111033.png]]
![[Pasted image 20251107111043.png]]
![[Pasted image 20251107110612.png]]

- Lowering the learning rate did help, but it looks like segment length of 24 is still too long for this case
- Let's try increasing the segment length

## v3 - segmentation strategy, no length strat, percent error loss
Difference from previous: decrease the last segment length
``` python
parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
loss:
![[Pasted image 20251107114322.png]]
![[Pasted image 20251107114332.png]]
![[Pasted image 20251107114051.png]]
- This didn't really help but we're not converged - let's increase training steps

## v4
Difference from last - increase steps_strat for phase 1
``` python
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

![[Pasted image 20251107121722.png]]
![[Pasted image 20251107121834.png]]
![[Pasted image 20251107115147.png]]
Interestingly, we do actually track a little better in the beginning with the longest segmentation ratio but then our errors explode. Let's try implementing a loss function that also penalizes mse.

## v5 - combined loss
Difference from last - we are using a loss combining percent error and mse. 
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200 ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error"
  loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

The loss funciton now combines percent error and mse. mse is scaled so it is on a similar scale to percent error, and incliudes a weighting factor for further control:
``` python 
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=1.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse
    ```
![[Pasted image 20251107123714.png]]
![[Pasted image 20251107123725.png]]
![[Pasted image 20251107123808.png]]
![[Pasted image 20251107123245.png]]
- This is tracking a bit worse, but from comparing the magnitude of the percent error vs the actual tracked loss, the mse is contributing very little. 
- It also looks like our model may not be sufficiently complex
	- ETA: this was not true - if we change the width and depth to 32 and 3 respectiely, our acceleration error is much worse (this is in run astral-wave 5159): ![[Pasted image 20251107125020.png]]
	![[Pasted image 20251107125037.png]]
	![[Pasted image 20251107125052.png]]
Let's try weighting the mse more heavily. 
Note - the mse weight should be a parameter in the config. I'll fix this later


## v6 - combined loss w/increased weight
Difference: same as before, but mse weight = 50
![[Pasted image 20251107125856.png]]
![[Pasted image 20251107125914.png]]
![[Pasted image 20251107130500.png]]

![[Pasted image 20251107125708.png]]


TODO:
- [ ] total loss, percent error, rms on the same plot
	- [ ] hope that we get tradeoff between loss functions - if not this may indicate that we are in a local minimum
	- [ ] replace staircase representation in val loss with interpolated val loss
	- [ ] test behavior with 1 elliptic orbit
	- [ ] simplify problem
	- [ ] John suspects activation function will help a lot - it looks like there is a critical radius where we go from being attracted to flying away. why do we go from attractor to repulsive force. we expect to see conical section still. 
	- [ ] accelerations should be negative in the radial direction - we could design to say a certain number must be negative. dot product between accel and position vector should be more than 90 degrees out of phase. show the direction of the acceleration vector. 

order of ops:
- simplify orbits
- change activation gelu, relu, leaky relu
- plot outputs for actual acceleration mag and direction
	- maybe new loss constraint - penalize if force isn't attractive
- expect to find a loss function that is suitable for different problems
- shouldn't really need to weight mse since everythign is already nondimensionalized

look at inputts as a function of time
John is concerned that position is contrained -1 to 1
plotting states as a function of time - true vs discrepant orbit 
input - velocity magnitude and an angle for that. or replace the velocity components completely.















##  OLD v3 - segmentation strategy, no length strat, combined loss, mse_weight = 10 (this was usi)
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error_plus_nmse"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

where the loss `percent_error_plus_nmse` is:
``` python
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=10.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse```

![[Pasted image 20251107095852.png]]
![[Pasted image 20251107095907.png]]
![[Pasted image 20251107100032.png]]



- [ ] we want to implement save best model, and ideally if we could do it by phase that would be even better
- [ ] what is a better way to test? right now we are visualizing each phase against a random validation trajectory
- [ ] add mse weight to the config
# November 5
Sick last couple days, trying to make up time.

To Do:
- [ ] better understanding of segmentation training curriculum
- [ ] 
We got the segmentation curriculum working in train.py and neuralODE.py. Transferred analogous changes to sweep.py and added some padding (but need to update to add padding for all strategies - I think I have this stashed in code or obsidian somewhere?)

On my quest to get my heat map fixed:
First, let's record changes that were made in order to plot different phases of training both for record keeping and to triple check that the behavior we are seeing in training is correct. 


testing new loss function:

# October 31
Fixing segment length strategy:
Let's move the segmenting portion of the data processing to within train_model so that we can keep the training script relatively clean. we can still load the total dataset into the main script and then 

We found that it is important to segment each training orbit into smaller segments when training our neuralODE. Otherwise, we were seeing vanishing gradients which we hypothesize is in part because the time horizon for integration is too long when we do autodiff. We now want to investigate if a training curriculum in regards to segmentation strategy would be useful, e.g. will we see improved training if we train on shorter segments then increase the lenght of these segments (though there is likely a point of diminishing returns). 

I want to be able to implement this in our scripts. We had a first pass but this put the strategy implementation within the main script, which isn't really in line with how we handle curriculum training in the case of length_strategy. Can we pass the config and original dataset to train_model, and do the segmentation within train_model based on our current phase of training or is that not the best idea? I still want to be able to log to wandb appropriately. For context, the git stash contains the previously recommended method of doing all of the dasta segmentation within the main, while I would prefer this happens within neuralODE.py l.ike the rest of our strategy work. 

Training is working as expected now in `train.py`. Let's double check `sweep.py`. Also check that metadata that is uploaded is correct now that we have a new method of training. I'll do a quick spotcheck.

I want to be able to visualize how each phase of the model training tracks an orbit. I think a good method to do this would be to load the dataset, average the orbital elements of each orbit in the dataset, and apply the model at these averaged initial conditions (but apply the model at the end of each phase of training if possible). Visualize the true orbit vs the model predicted orbit in a manner similar to Integration Visualizer at each phase, and plot acceleration statistics.
# October 29
Finishing conda-pack and unpack
# October 28
I am getting Zaratan set up so I can run large numbvers of training scripts.

Ideas for improving training:
- try new coordinates
- different loss functions
- different input and output layers
- length strategy
- segmentation curriculum learning

## Zaratan
`module load:`
- `module load <modulename>` temporarily modifies your environment so you can use specific software packages or versions without needing admin rights
	- ex. `module load python` loads python 3.10.10
- we need to use `module load` each time we start a new session unless it is automated in .bashrc
- university recommends that `module load` commands do not go in .bashrc, rather just put them in the submitted bash script

Slurm:
- `squeue --me` shows queued jobs in pending state
- `sbalance` shows account balance (I think this is shared across the lab)
- each core on Zaratan is 4GB mem
- `#SBATCH --ntasks=<n>` specifies the number of parallel tasks (processes) to launch - you would use this if running multiple instances of your program (so this would be good for me)
- `#SBATCH --cpus-per-task=<n>` is the number of cpu cores assigned to each task. Use this for multithreaded programs (like numpy) that need multiple cores per process

Getting my conda env set up:
- I have project dependencies - how do I know if these are installed on zaratan and how can I install things that are needed? I think that's the point of the conda env - I install in there in my active conda env
- I'm pretty sure that I ended up needing to pip install a few more things - how can I check curiosity to see what I have?
- am I able to change branches when I run this stuff?
-  
- can I pip install my project? I think I will have to pull the git repo into my scratch directory? do I pip install my project in my conda env?
- steps I think I will take:
	- git clone
	- pip install project ?
	- I'm also going to need to install mldsml and switch to the neuralODE-dev branch
	- is it necessary to pip install things from the pyproject.toml when it may be taken care of by just pip installing the project?
	- am I actually able to use wandb to pull datasets down from hpc? wandb on zaratan isn't communicating online but I imagine it can still pull it?
- how do I check the available memory in my scratch directory? I think I might have to store datasets on zaratan directly
- I'm going to have to pull files down from wandb. Is there a way I can pull them down and save them off locally so that we can use datasets that are already created?
- there are data transfer nodes on zaratan - how do I access them?

I am training a neuralODE. I have a git repo with many files, folders, etc. I have config.yaml files that are read in to the main training script, and these config files contain different parameters (like model size, loss function, etc). I want to run many different instances of the training script with different parameters based on these config files, so I am trying to get the code set up to run on my university's HPC. I am creating a conda environmnt to install all required dependencies. I also use another git repository central to my lab which I will have to pull down and pip install. I am looking for advice on how to get everything up and running. A few questions:
- can I pip install the repositories into the conda environment? will the dependencies also be installed?
- am I able to switch branches when I am running things on HPC? I need to be on a different branch of my lab repository
- will pip installing these projects automatically install the dependencies?

do NOT run jobs out of home directory
home directory is the only place where files are backed up - scratch and SHELL are not

```
source ~/scratch/miniforge3/etc/profile.d/conda.sh

conda activate env_name

```
# October 21

I am training neural ODEs to learn 2BP dynamics, and for simplicity I am beginning with the planar case. Through my research, I found that it is very important to each orbit in the training dataset into smaller trajectories because otherwise we see vanishing gradients (hypothesized that when we use the adjoint sensitivity method to compute the gradients, the integrand is overly stable such that when we integrate over time horizons that are too long, we get vanishing gradients). We still think length strategy is important, which refers to training on a portion of the time series of the data to warm start the model and decrease computation time, but in this plot we are just training on the entire time series. 

In this heat map, we are showing the effects of training on different numbers of randomly generated orbits and the effects of segmenting those training orbits into different numbers of segments. When we generate random training data, we set a range for each orbital element and then randomly select from there so our orbits range from LEO to GEO, circular to as eccentric as possible without intersecting the earth. The segmentation ratio indicates what percent of the total trajectory each segment makes up (e.g. 5% means that we segmented the orbit into 20 trajectories, each being 5% of the total orbit). We then test the model on datasets containing different numbers of randomly generated orbital initial conditions, apply the model to the random initial conditions, and find the percent error in the true vs model predicted acceleration averaged over all timesteps and orbits in a given test dataset.

The segmentation trends are generally as expected - the smaller the segment and shorter the time horizon for integration, the lower the error when applying the model to a test dataset. However, we don't see the expected monotonic trend that increasing number of training orbits decreases model error. 

For now, we are focusing on getting the expected smooth gradient in the bottom 3x3 (or 4x4 if necessary) grid of the heat map. Some ideas we've had:
1) to trouble shoot, try to simplify by first testing on ciruclar orbits with different sma. However, this might not be a good idea because when we learn 2BP dynamics we are basically learning the 1/r^2 term, so circular might actually be harder to learn? for context, this is our input layer:
``` python

def sph_4D_rinv_vel(y):
    pos = y[:3]
    radius = jnp.sqrt(jnp.sum(pos**2))
    return jnp.concatenate(
        [
            jnp.array(
                [
                    1 / radius,
                    pos[0] / radius,
                    pos[1] / radius,
                    pos[2] / radius,
                    *y[3:],
                ],
            ),
        ],
    )
```

and this is our output layer:
``` python
def mlp_4D(mlp_output, state, scalar=1.0):
    r_mag = mlp_output[0:1]
    r_dir = mlp_output[1:4]
    acc_pred = r_mag * r_dir
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
2) should we try curriculum training with segmentation strategy? we get better results when we start with small segments, but the loss curves are quite noisy (we should also test different learning rate). maybe train first on a smaller segmentation strategy and then increase it?
3) are our input and output layers sufficient?
4) are the other parameters sufficient?
5) We currently start with a random true anomaly at the first timestep of each orbit (before segmentation) - is this necessary?
---

Let's test on a few different datasets:
**sensitivity-2D-baseline-v1:**
This tests on randomly generated orbits with the following possible ranges/params:
- SMA = \[R_Earth + 200km, R_GEO\] (where GEO is 42164 km)
- e = \[0, 0.99\] (in reality we check for intersections, so the max e is the largest eccentricity without intersecting the earth)
- planar (i = 0, RAAN and argument of periapsis undefined)
![[orbital_elements_hist_baseline 1.png]]
![[multi_dataset_xy_baseline 1.pdf]]

![[heatmap_data_sensitivity_baseline.png]]

![[W&B Chart 10_21_2025, 11_13_27 AM.png]]

![[W&B Chart 10_21_2025, 11_14_09 AM.png]]
**sensitivity-2D-fixednu0-v1**
let's generate datasets using the same random initial conditions but now nu0 = 0 for every dataset. naming scheme will be complex_TBP_planar_fixnu0_{num_orbits}

![[multi_dataset_xy_fixnu0 1.pdf]]
![[orbital_elements_hist_fixnu0.png]]
Now let us train the model
Testing on the baseline datasets with not fixed nu0:

![[heatmap_data_sensitivity_fixnu0.png]]

Testing on fixed nu0:
![[heatmap_data_sensitivity_fixnu0_testfixnu0.png]]


![[W&B Chart 10_21_2025, 11_12_54 AM.png]]
![[W&B Chart 10_21_2025, 11_12_25 AM.png]]

**sensitivity-2D-circular-v1**
This tests on circular orbits with random SMA:
![[multi_dataset_xy_circular.pdf]]

![[orbital_elements_hist_circular.png]]
Now let's train:
![[W&B Chart 10_21_2025, 11_32_15 AM.png]]

![[W&B Chart 10_21_2025, 11_32_57 AM.png]]

Testing on original datasets:
![[heatmap_data_sensitivity_circular_testbaseline.png]]
Testing on circular (training) datasets:
![[heatmap_data_sensitivity_circular.png]]























































# October 16
## Recap
We are working on 2D sensitivity analysis. Preliminary results were surprising - we were expecting to see a monotonic trend that as the number of training orbits increases, the error decreases and that as the segment ratio decreases, the error decreases. The latter is true but the former is not:
![[Pasted image 20251016122314.png]]
![[Pasted image 20251016122305.png]]
We are trying to figure out why this is happening, and want to focus on getting a nice gradient while just looking at the lower left 3x3 or 4x4 grid. 
We also noticed that the loss curves associated with this training are very noisy:
![[Pasted image 20251016122832.png]]
![[Pasted image 20251016122946.png]]
This is probably due to a variety of parameters and hyperparameters. Learning rate may be too high, segmentation strategy may put us in a local minimum that we can't get out of (maybe? double check notes from meeting), might need a training curriculum, etc. Try to get the bottom 3x3 or 4x4 of the heat map behaving expected by testing the interactions of these parameters, implementing training curriculum, etc.

## Sensitivity Analysis
Getting to the bottom of the issue described above...
The issue that we are currently investigating is that as we increase the number of training orbits, our results are not monotonically improving.

### Training data
Let's look at what our training datasets were for the first 4x4 grid:

**TBP_complex_planar_1:**
\[a, e, i, omega, w, nu\] = \[3.5359730e+04 3.7370604e-01 0.0000000e+00           nan           nan
 5.6238999e+00]\
 SMA = 35359.73 km (GEO is ~42 km)
 e = .3737
 nu0 = 322.23 deg
 ![[Pasted image 20251017124525.png|200]]
 
TBP_complex_planar_4:
![[Pasted image 20251017125450.png]]
Note to self - verify that I apply models to the test datasets
 
# October 7
## Recap
- I've begun work on 2D sensitivity studies but need to get more experiments and visualizations up and running. We need to start using the "num_trajs" parameter again for analysis.
- Finished the ASTRA poster yesterday. I made some modifications to the 2BP vector field visualization and experiment that should be documented. 
- I'm having some memory issues when I work locally, but I am having some issues with Docker and installing packages using `pip install -e .`
- I noticed that change $\nu_0$ for the test orbit after training 3D model changes the results largely, which is a bit perplexing since we also segment data. I want to investigate this. 

## Goals
- [x] Fix Docker issues
- [ ] Clean and document vis and experiment from yesterday
- [ ] 2D sensitivity studies. At a min, study num_traj vs segment_length
- [x] quick 3D study so we can talk about it in meeting
- [ ] there is one more stash from the conference I have to go through and apply/document
- [ ] prevent wandb files, artifacts, etc. from being saved locally
## Sensitivity studies
Step 1 is being able to access the parameters we want to study! For now, these include:
- [ ] number of training orbits
- [ ] segmentation strategy
- [ ] length strategy
Segmentation strategy and length strategy are already easily accessible. We also previously utilized `num_trajs` param, but this has basically been phased out in favor of training using the explicit names of datasets defined in the config. So, we can either consistently name datasets and put the num_trajs as a variable within the name (e.g. "simple_TBP_planar_5"), we can dynamically find the number of trajectories by loading the data, we can include another num_trajs param to weep over (but we would have to make sure that we have it properly corresponding to the correct dataset), or we can add a parameter to the dataset itself indicating the number of trajectories. The last option is probably our best bet since reloading data unnecessarily is expensive. 

Basically, this just means that when we train the model we should record how many orbits the model was trained on (taking into account train_test_split). We already have to load the data for training of course, so at that point we can record the number of training orbits to wandb.

I updated the training script and model save function so that we can pass optional additional arguments to log to the model. Currently, I added segment_ratio so we have direct access to the percentage each segment is of the total trajectory, as well as num_total_orbits and num_train_orbits so we don't have to do any additional data manipulation post model training.

Let's run a quick train script and make sure everything is working as expected.

Note - training is taking a very long time to run locally through docker because a lot of space isbeing used on saving artifacts, models, etc. We don't want to do this since everything is just uploaded to wandb and we can pull it down later. 



# October 6
## Recap
Working on sensitivity studies for 2D scenarios (segmentation, amount of training day, length strategy in particular)
## Goals
- [x] Finish ASTRA poster
	- [x] vector field diff plot overlaid with training orbits for a quick glance of how the training data amount impacts training
- [x] Figure out why 3D training isn't working well on test orbit (kinda done)
## 3D training
Last week, I began training on 3D orbits (as opposed to the planar ones we used for the conference). The segment length is 10, i.e. the trajectory is split into 36 segments before training. The loss curves are converging to acceptable values:
![[Pasted image 20251006110049.png]]
I did a sanity chck by testing on a random orbit that fell within the bounds of the training data and got very bad results:
![[Pasted image 20251006110119.png]]
During group meeting, Kruti asked if I had tried differ initial conditions. I had tried altering the shape of the orbit and got similarly bad results, but I had not altered the initial true anomaly.  When I change nu0 from 0 to 20 degrees, I get much better results:
![[Pasted image 20251006110526.png]]
![[Pasted image 20251006110545.png]]


What does this indicate? When we generate data, we take a random initial true anomaly from 0 to 360 degrees. The other orbital elements are randomized according to our desired dataset. When we load the data in, we segment the trajectory. Because of this segmentation, I am surprised that the initial true anomaly would have such a large impact since the model is functionally exposed to *many* different initial true anomalies through segmentation. 
This leads me to wonder: 
1) What does the full predicted vector field look like? How does this change as a function of randomizing vs not randomizing the initial true anomaly (e.g. will results be significantly different if we just always have an initial true anomaly = 0)? If we keep nu0 of the full trajectory randomized, how does the amount of training data affect the model accuracy? We would expect that increased exposure to more ICs, i.e. more training data, would improve accuracy.
2) How important is segmentation in 3D? We expect that it will have a larger effect than in 2D due to the increased dimensionality of the problem - to what extent is this true?

experiment idea for 2d: plot the residual of the vector field against the training orbits

## ASTRA Poster
I want to create the following visualization:
Train on 10 random 2D orbits (complex case). Use all orbits for training, i.e. train/val split = 1. Plot a vector field with the residual of the true versus predicted acceleration, overlayed on top of the training orbits.
- [ ] Generate training data
- [ ] Visualize
- [ ] Update yaml
- [ ] Update train script (use correct yaml)
- [ ] Train
- [ ] Visualize


