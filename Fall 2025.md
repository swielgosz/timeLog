# December 4
Recap:
- We discovered a failure mode where predicted acceleration direction can flip 180 degrees. Our mlp output was acceleration value and its direction, but this is not a good output because $a\times\hat{a} = -a\times -\hat{a}$. We hypothesized that fixing this would result in better training and/or generalization, but so far we have not been seeing this.
- This is in part because if we only modify the output layer to be |a| and $\hat{a}$, we now have the correct sign of direction but the model could predict a or -a and get the same output, which may confuse the network - non-uniqueness might be causing trouble. abs value is also non differentiable. We switched to softplus(acc) and see smoother convergence than either signed or abs acc results, but losses are still higher than signed which is perplexing. However, note that the aforementioned failure mode was not seen. I want to figure out when this failure actually occurs.
	- We may want different activation functions or feature representations
- In general, regardless of loss function we would expect the acceleration to be better than it is
- We often see that acceleration begins to point radially relative to its true direction , peaking at periapsis. The predicted acceleration is still opposite the predicted direction, i.e. the predicted theta is 180 degrees, but predicted acceleration is radial compared to true acceleration vector. Acceleration direction changes most quickly at periapsis so I am not surprised it is having trouble here, but I would like to mitigate this. 

## Testing input/output features, activation functions
Is there a better final activation we can use? Maybe better input or output features?
We originally had our mlp produce 4 outputs, which were taken to be the acceleration's value and its unit vector components in x, y, and z. However, this was problematic because $a\times\hat{a} = -a\times -\hat{a}$. We hypothesized that fixing this would result in better training and/or generalization, but so far we have not been seeing this. We tried our output layer taking the absolute value of the first component to ensure that acceleration is positive, but this is problematic because a) absolute value is not differentiable and b), the model could predict a or -a and get the same output, which may confuse the network - non-uniqueness might be causing trouble. We used softplus instead since it is unique to ensure that acceleration always has a positive value.

Questions:
- Currently, a final activation of softplus is applied to the acceleration value, and tanh can be applied to each of the unit vector components but is not for the baseline comparison. Are there better final activations?
- Are we using the best input features? We use 1/r, x/r, y/r, z/r, vx, vy, vz. I think x/r y/r z/r essentially equates to polar coordinates? Maybe not - draw this out
- Are we using the best output features? We output the acceleration value and its direction components. We know this because we take acceleration = a*\hat{a}, and this acceleration vector is what represents the dynamics and gets integrated to compute loss
- Is there a way we can avoid the acceleration turning radial?

Changes and ideas:
- First, let's change our loss back to percent error + rmse. This has been shown to consistently give better results than percent error.
- Re: activations:
	- Magnitude activation: keep positivity but improve conditioning by predicting log_acc and mapping with jnp.exp(log_acc) or jnn.softplus(log_acc) + eps. Training the network on the log of the target magnitude often stabilizes large dynamic ranges and removes the sign ambiguity you saw with abs. If you know a reasonable max accel, a sigmoid scaled to that range can also help.
	- Direction activation: normalize the direction vector explicitly (use mlp_4D_unit/mlp_4D_unit_scaled) and add a small epsilon to the norm to avoid NaNs; optional tanh before normalization to prevent exploding norms. This removes the magnitude–direction entanglement and the ± ambiguity.
		- why would we tanh before normalization?
- Re output features:
	- Orthogonal basis: instead of x/y/z components, output acceleration in a physically defined basis such as RTN/RSW (radial, transverse, normal). Build the basis from the current state, predict three unconstrained scalars, then rotate back to Cartesian; this fixes the sign convention because the basis is uniquely determined by (r, v).
- Re: input features: 
	- we currently use direction cosines, inverse radius, and velocity
	- consider adding - Radial and transverse velocity: vr = <r_hat, v>, vt = |r×v|/|r| (and optionally the normal component), so the network sees how motion splits along/orthogonal to r_hat.
		- is radial and normal not the same?
Overall suggestions: if you stay with magnitude+direction, normalize the direction and learn log‑magnitude; if you switch to RTN components, you can output three unconstrained scalars and skip special activations. If simplicity wins, predict the Cartesian acceleration directly (3 outputs) and drop the magnitude/direction split—the solver still receives the correct vector without ambiguity. Natural next experiments:
1. Swap to a log‑magnitude head (mag = exp(log_mag)) plus direction normalization with epsilon.
2. Try RTN outputs using a basis from (r, v) and predict a_r, a_t, a_n.
3. Add vr/vt (and maybe |h|) to the feature layer and see if generalization improves.


Our hope is to improve generalization ability while training on fewer orbits. Let's get some baseline results. Training on 10 complex orbits, testing mlp_4D and mlp_4D_softplus (where softplus is only applied to the acceleration output):
config:
``` python
wandb:
  group: "complex-lofi-debug"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[1000]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
 **mlp_4D_signed with percent error loss:**
 run_id: snw96q4r
 runtime: 1:32
 
**mlp_4D_accsoftplus with percent error loss:**
run_id: weirulst
runtime: 1:39

**mlp_4D_signed with percent error plus rmse loss:**
run_id: vejpt8p8
runtime: 1:55

**mlp_4D_accsoftplus with percent error plus rmse loss:**
run_id: 06n0t40m
runtime: 1:53

The following results are the average acceleration error on separate validation datasets of 1, 10, 100 orbits:

| label    | output        | loss          | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------- | :------------ | :------------ | :-------------- | ------------------------: | -------------------------: | --------------------------: |
| snw96q4r | mlp_4D_signed | percent error | snw96q4r        |                      4.61 |                       5.93 |                        5.38 |
| weirulst | softplus      | percent error | weirulst        |                      10.4 |                       10.5 |                        9.41 |
| vejpt8p8 | signed        | rmse          | vejpt8p8        |                      4.26 |                        6.3 |                        5.08 |
| 06n0t40m | softplus      | rmse          | 06n0t40m        |                       7.2 |                       9.99 |                        8.82 |

We'll go forward with the percent error + rmse loss. 

Trying a few new output layers
### 
### mlp_4D_logmag_unit_tanh_exp
output layer:
``` python
def mlp_4D_logmag_unit_tanh_exp(mlp_output, state, scalar=1.0, eps=1e-8):
    """Use log-accel magnitude with exp; tanh + normalize for direction.

    The MLP predicts log(|a|); we exponentiate to ensure positivity. Direction
    logits are squashed to [-1, 1] and normalized to unit length with an epsilon
    guard. `scalar` can be used to rescale the magnitude if desired.
    """

    log_r_mag = mlp_output[0:1]
    r_mag = scalar * jnp.exp(log_r_mag)

    r_dir_logits = mlp_output[1:4]
    r_dir_bounded = jnn.tanh(r_dir_logits)
    r_dir_mag = jnp.linalg.norm(r_dir_bounded) + eps
    r_dir_unit = r_dir_bounded / r_dir_mag

    acc_pred = r_mag * r_dir_unit
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
run_id: 0z7kzz6g
runtime: 1:44

| label    | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------- | :-------------- | ------------------------: | -------------------------: | --------------------------: |
| 0z7kzz6g | 0z7kzz6g        |                      4.25 |                       10.9 |                        8.46 |
the loss looks really smooth! it isn't quite converged though - let's let it run a bit longer
![[Pasted image 20251204172614.png|500]]
Let's run for 2000 steps (rather than 1000) and check in:      
run_id: y13uegx3
runtime: 3:33
![[Pasted image 20251204173227.png|500]]
looking much more converged! but generalization isn't a ton better:

| label    | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------- | :-------------- | ------------------------: | -------------------------: | --------------------------: |
| y13uegx3 | y13uegx3        |                      4.35 |                       10.4 |                        7.94 |
### mlp_4D_logmag_unit_tanh_softplus
output layer:
``` python
def mlp_4D_logmag_unit_tanh_softplus(mlp_output, state, scalar=1.0, eps=1e-8):
    """Log-accel magnitude mapped with softplus; tanh + normalize direction.

    Softplus grows more gently than exp, which can improve conditioning while
    still enforcing positivity. `scalar` optionally rescales the magnitude.
    """

    log_r_mag = mlp_output[0:1]
    r_mag = scalar * (jnn.softplus(log_r_mag) + eps)

    r_dir_logits = mlp_output[1:4]
    r_dir_bounded = jnn.tanh(r_dir_logits)
    r_dir_mag = jnp.linalg.norm(r_dir_bounded) + eps
    r_dir_unit = r_dir_bounded / r_dir_mag

    acc_pred = r_mag * r_dir_unit
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
run_id: 7zmyhy53
runtime: 1:59
not converged, looking pretty similar to the previous after 1000 steps (this one in yellow):
![[Pasted image 20251204173753.png|500]]
# December 3
Recap:
We are currently working on investigating the effects of having fixed the mlp output. We suspect that if we have a corrected output, we will improve generalization abilities. Yesterday, we tested if the new pipeline improved generalization ability of the model. We trained on 100 random complex orbits, and tested on a separate validation set of 100 random complex orbits. We first tested this by comparing results of training using the conference hifi config vsThe new pipeline did slightly improve generalization, and notably the lofi config was as good or better than the hifi config. I think this is mainly because lofi config switched to leaky relu. Let's see if this is true - if we change just the activation function, what happens?

TODO:
- [ ] plot datasets
- [ ] train with new and old pipeline on 1 and 10 complex orbits to compare ability to generalize
- [ ] try lofi with new and old pipeline with tanh vs leaky relu
- [ ] investigate difference in convergence times
- [ ] implement at least one visualization that John will like
- [ ] can segment length make dynamics linear? how can we test this?
- [ ] what is the effect of activation on output layers?
- [ ] how will different input/output layers affect training?
- [ ] add flag for cpu vs gpu
- [ ] figure out how to access runtime for wandb
- [ ] box and whisker plot visualization?
- [ ] brainstorm visualizations/sensitivities we want to run
- [ ] try segmentation length sweep
- [ ] implement basic latent ODE?
- [ ] after hours - clean up commit history


knobs to turn:
1. leaky_relu vs tanh
2. batch size
3. output layer and activation
## comparing lofi results
We want to see if fixing the output layer allows us to use a less handholdy training method. To test this, let's run the following tests:
1. Use the same parameters as the conference hifi config *except* do less steps and no length strategy. Use incorrect output
2. Use the same parameters as the conference hifi config *except* do less steps, no length strategy, and leaky_relu instead of tanh. Use incorrect output
3. Use the same parameters as the conference hifi config *except* do less steps and no length strategy. Use correct output
4. Use the same parameters as the conference hifi config *except* do less steps, no length strategy, and leaky_relu instead of tanh. Use correct output

### case 1
run_id: 76h805fh
runtime: 1:12
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-tanh"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
### case 2
run_id: bxlii4iv
runtime: 2:26
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-leaky_relu"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
why does leaky_relu take longer to train? this may just be a computer/wandb related thing/ why is my cpu usage 328%?
### case 3
run_id: rvkedxfy
runtime: 1:21
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-tanh"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

### case 4
run_id: eb38i1t2
runtime:
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

What we are trying to determine here is 1. is leaky relu significantly better than tanh, 2 is abs val output significantly better than signed output and 3 are these related

Metric: acceleration eror of model applied to test dataset
Training on 100 complex orbits:

| label                                    | wandb_group_id                           | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :--------------------------------------- | :--------------------------------------- | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed-pe-tanh       | complex-lofi-mlp_4D_signed-pe-tanh       |                      11.7 |                       15.6 |                        11.8 |
| complex-lofi-mlp_4D_signed-pe-leaky_relu | complex-lofi-mlp_4D_signed-pe-leaky_relu |                      2.34 |                       2.11 |                        1.98 |
| complex-lofi-mlp_4D-pe-tanh              | complex-lofi-mlp_4D-pe-tanh              |                        19 |                       20.1 |                        16.3 |
| complex-lofi-mlp_4D-pe-leaky_relu        | complex-lofi-mlp_4D-pe-leaky_relu        |                      2.18 |                       2.72 |                        2.18 |

**Conclusions:**
- leaky_relu is sooo much better!! why is this? Let's poke at this a little bit but not dwell on it too much for now. 
- signed vs unsigned acceleration in the output layer doesn't have a major output on the overall results IN THIS CASE - the model is much more obviously improved by using leaky relu instead of tanh. I'm curious if it has an effect on convergence rate or output tracking though. Lets plot the losses and whatnot for the cases above. Also, we need to see if the results are different for when we train on fewer orbits! Let's repeat the test from yesterday for the other amounts of orbits

It doesn't look like there is a significant difference in convergence/noisiness using mlp_4D vs mlp_4D_signed

For Neural ODEs, smoothness and well-behaved derivatives often matter more than in standard nets because the ODE solver tracks the vector field continuously.

- Prefer smooth, non-saturating activations: Swish/SiLU, GELU, Softplus, ELU. They keep gradients flowing yet give a smooth vector field, which can reduce solver jitter and step count.
- ReLU/leaky ReLU work but introduce kinks; they’re usually fine, but very stiff dynamics can force smaller solver steps. If you notice step explosion or instability, try Softplus or Swish.
- Avoid hard saturation (tanh/sigmoid) unless you really need bounded dynamics; they can make the vector field flat and slow training.
- If you like leaky ReLU but want smoother negatives, PReLU (learned slope) or Softplus- shifted (e.g., softplus(x) - ln(2)) are easy swaps.
- Keep the final layer linear/identity so outputs can be negative; the hidden activation choice doesn’t prevent that.

## training on 1 complex orbit
### Abs value acceleration output
run_id: 2o6ir5kx
runtime: 0:48
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu-1"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_1_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
### Signed acceleration output
run_id: zclmkvzm
runtime: 0:49
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed_-pe-leaky_relu-1"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_1_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

| label                                       | wandb_group_id                              | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------------------------------------ | :------------------------------------------ | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed_-pe-leaky_relu-1 | complex-lofi-mlp_4D_signed_-pe-leaky_relu-1 |                       517 |                        433 |                         360 |
| complex-lofi-mlp_4D-pe-leaky_relu-1         | complex-lofi-mlp_4D-pe-leaky_relu-1         |                       459 |                        367 |                         299 |
## training on 10 complex orbits
### Abs value acceleration output
run_id: bhehtoo2
runtime: 1:54
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
![[Pasted image 20251203151828.png]]
![[Pasted image 20251203151839.png]]
![[Pasted image 20251203151844.png]]![[Pasted image 20251203151807.png]]
![[Pasted image 20251203151740.png]]
![[Pasted image 20251203151756.png]]
### Signed acceleration output
run_id: rspv9fjn
runtime: 2:10
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
![[Pasted image 20251203152033.png]]
![[Pasted image 20251203152039.png]]
![[Pasted image 20251203152046.png]]
![[Pasted image 20251203152054.png]]
![[Pasted image 20251203152101.png]]
![[Pasted image 20251203152116.png]]

| label                                       | wandb_group_id                              | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------------------------------------ | :------------------------------------------ | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed-pe-leaky_relu-10 | complex-lofi-mlp_4D_signed-pe-leaky_relu-10 |                      4.61 |                       5.93 |                        5.38 |
| complex-lofi-mlp_4D-pe-leaky_relu-10        | complex-lofi-mlp_4D-pe-leaky_relu-10        |                      14.7 |                       10.6 |                        8.16 |
- Hmm this isn't really what I expected to see? I wonder if this is because we are using a longer segment length again (length = 18/360 = 5% of orbit) - we don't see the flip flopping in this case. However, if we have a shorter segment length then the output layer may be more important. 
- Also, this is when we have a percent error loss function.
- What I am seeing based on the output feature plots is that if we predict the incorrect sign of acceleration magnitude, we *also* predict the incorrect sign of acceleration direction vector in a consistent manner that results in a_mag_signed\*a_direction being accurate. Previously, the failure mode that we were seeing was acceleration signed value and its direction randomly flipping. This could have just been when tanh was the activation? or when the segments were shorter? Or when there were less training orbits? Not sure yet. Let's investigate the output features of training on one orbit at least to see if any flipping occurs there. We are also only looking at the models applied to the training orbits. 
- Looking through results, the most common failure currently is that near periapsis, acceleration begins to point radially:
- ![[Pasted image 20251203152805.png]]
This doesn't always happen, but certainly is not uncommon. 
We also haven't completely converged - maybe if we train longer there is some point of diminishing returns?
Let's train for 5500 steps and see if this changes generalization capabilities of mlp_4D vs mlp_4D_signed

run_id ye0p3i6q (pleasant-breeze-5754)
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D_signed-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[[0.0,0.1],
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, 0.001]]
  steps_strategy: [[500, 5000]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
``
run_id ktnrm7na (avid-dragon-5753) 
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-pe-leaky_relu-10"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[[0.0,0.1],
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, 0.001]]
  steps_strategy: [[500, 5000]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

these spikes in the train loss are interesting
blue is mlp_4D_signed output
yellow is mlp_4D
these still don't look totally converged BUT it looks like mlp_4D converges faster
train loss is lower for mlp_4D_signed, but this doesn't speak to generalization
Let's see how these generalized
![[Pasted image 20251203154343.png]]

| label                | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------------- | :-------------- | ------------------------: | -------------------------: | --------------------------: |
| yellow - mlp_4D      | ktnrm7na        |                      18.1 |                       12.4 |                        9.72 |
| blue - mlp_4D_signed | ye0p3i6q        |                      7.01 |                       9.41 |                        7.68 |
|                      |                 |                           |                            |                             |
Based on this, it doesn't look like the corrected output helps us. This is surprising to me. Some thoughts:
- model wasn't converged
- convergence looks to occur faster with mlp_4D vs mlp_4D_signed, but is notably noisier for mlp_4D than mlp_4D_signed
- I'm not sure if this would change when we're doing training on 3D data

I'm confused. A couple weeks ago, I found a failure model in my model. my output layer that I was previously using was mlp_4D_signed, where the output of the model consists of an acceleration value and direction, and the value could be signed. These values get multiplied together to produce an acceleration vector, which is then integrated to calculate loss. So, if the sign of the acceleration and its direction were both wrong, we could still get the correct acceleration vector. I noticed that sometimes the output features would randomly flip signs (i.e. the sign of the acceleration valu and the sign of its direction vector would both flip) but it wouldn't matter because the product of them was still the same. However, it seems like this would make learning more difficult for the model, make it harder to generalize, etc. I was expecting that if always took the absolute value of the acceleration, the direction would always be correct and the model would train and generalize better. While the direction is now properly signed, I am getting results that show a lower training loss and better generalization capabilities when I use the mlp_4D_signed output layer rather than mlp_4D (which has the absolute value of acceleration). This is surprising to me - do you know why this would be the case?

Possibilities
- Optimization landscape: forcing |acc| (mlp_4D) makes the loss non-smooth at 0 and removes the sign degree of freedom. The model now has to encode sign entirely in the direction unit vector, which can be harder early in training. With signed, the network can “spread” sign errors between magnitude and direction; gradients can point in more forgiving directions, so it converges faster and finds better minima.
- Representation flexibility: signed lets the model represent antipodal directions with a sign flip in magnitude, keeping direction vector near a stable mode. With abs, if the target direction flips (e.g., close to periapsis where acceleration direction changes rapidly), the network must swing the direction vector itself; this can be harder for a shallow/limited-width head.
- Gradient flow near zero: abs has a kink at 0; if your predicted magnitudes hover near zero early on, gradients on r_dir can be weak or noisy. The signed head gives smooth gradients through 0 (just linear), aiding learning.
- Implicit regularization: the extra symmetry in signed acts like a redundancy that can help optimization even if it doesn’t change the final physics. Enforcing a specific factorization (positive mag × unit dir) can overconstrain the model if the MLP isn’t strong enough or the data is noisy.
- Normalization/targets: if your target accelerations are noisy or have sign ambiguity (e.g., due to normalization or masking), the signed head might absorb that better.

- Even if your loss on the _acceleration vector_ is smooth (e.g., MSE between predicted and true accel), the mapping from the MLP’s raw magnitude m to the vector uses abs(m). The composite loss(abs(m) * dir(...)) has a kink where m crosses 0: the derivative w.r.t. m jumps from −∂loss/∂(acc) to +∂loss/∂(acc). That makes the overall loss non-smooth in the raw parameters.
- A kink in the loss shows up to the optimizer as a jump in the gradient direction. That can matter because:

- Gradient-based methods assume smoothness to make consistent steps; at a kink, the derivative is undefined or discontinuous, so updates can bounce or stall.
- Subgradient choices can flip sign across tiny moves (e.g., when m crosses 0), injecting noise and slowing convergence.
- Adaptive optimizers (Adam/Adagrad) treat sudden gradient changes as volatility, which can shrink steps or make them erratic.
- Second-order intuition (curvature) breaks near kinks, so momentum/step-size heuristics become less reliable.

A smooth positive map (softplus/exp) keeps gradients consistent, typically improving stability and speed, even if the true target never hits the kink.

Even when the loss is on the integrated Cartesian state, the computation graph includes abs(m) inside the ODE right-hand side. The overall mapping weights → m → abs(m) → acceleration field → integration → state → loss is a composition that inherits the non‑smooth kink at m=0. JAX will pick a subgradient there, but the derivative w.r.t. m flips sign across 0, so the loss as a function of the raw parameters is only piecewise smooth. In practice, if m doesn’t cross zero during training you won’t feel it, but when it does (early training or hard examples), the optimizer sees that discontinuity. A smooth positive map (softplus/exp) removes that kink while leaving the rest of the pipeline unchanged.

What to try:

1. Smooth the magnitude constraint: use softplus (maybe shifted) instead of hard abs so gradients don’t kink at zero.
2. Normalize the direction: if sticking with positive mag, ensure r_dir is normalized (unit) so the model isn’t juggling both mag and direction in that vector.
3. Add a small penalty for sign flips between steps if temporal consistency matters.
4. Compare width/depth or LR tuned for the constrained head; you may need a different LR when using abs.
5. Log mag and direction separately to see where the constrained head struggles (e.g., near zero crossings).

In short: the constrained factorization (positive mag + direction) changes the optimization geometry; the signed head’s extra symmetry and smooth gradients likely make training easier even if the final vector is the same.
## shorter segments
So the reason I started delving into analysis of the outputs of my mlp was originally because we were not seeing montonic trends of improvement as we increased training data and decreased segmentation length. Perhaps if we have a shorter segment length, this is when we see the flip flopping failure mode and that could be why we weren't seeing monotonic trends? Let's go back to training with segment length = 4 and see what happens when we train with lofi with mlp_4D vs mlp_4D_signed

first train with segment length = 4, acceleration is signed:
run_id: apnidf8l
runtime: 1:11
``` python
wandb:
  group: "complex-lofi-debug"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_signed
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

now train with segment length = 4, acceleration output is not signed:
run_id: 57mev9gj
runtime: 1:06
config: 
``` python
wandb:
  group: "complex-lofi-debug"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

What do we discover from this? does generalization change now?

| label         | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :------------ | :-------------- | ------------------------: | -------------------------: | --------------------------: |
| mlp_4D_signed | apnidf8l        |                      4.54 |                       6.26 |                        5.76 |
| mlp_4D_abs    | 57mev9gj        |                      13.2 |                         10 |                        7.66 |
|               |                 |                           |                            |                             |
let's change abs to softplus incase we are encountering any kinks and ssee what happens:
run_id: uhm3m4dc
runtime: 1:10
config:
``` python
wandb:
  group: "complex-lofi-debug"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_10_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_accsoftplus
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

and the output layer is:
``` python
def mlp_4D_accsoftplus(mlp_output, state, scalar=1.0):
    r_mag = jnn.softplus(mlp_output[0:1])
    r_dir = mlp_output[1:4]
    acc_pred = r_mag * r_dir
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
```

| label          | wandb_source_id | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |     |
| :------------- | :-------------- | ------------------------: | -------------------------: | --------------------------: | --- |
| mlp_4D_signed  | apnidf8l        |                      4.54 |                       6.26 |                        5.76 |     |
| mlp_4D_abs     | 57mev9gj        |                      13.2 |                         10 |                        7.66 |     |
| mlp_4D_sigmoid | uhm3m4dc        |                      11.8 |                         11 |                        9.63 |     |
## John Report:
Status update:
1. leaky_relu is so much better than tanh. I've been using leaky_relu for the past month+, but here's a numerical example of how much better it is. When we train on 100 complex orbits for 1000 steps, each segment = 5% of the orbit, no length strat, we get the following average acceleration errors on independent validation sets with 1, 10 and 100 orbits respectively:

| config label                             | Acceleration output - signed (raw output) or abs val | activation function | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :--------------------------------------- | :--------------------------------------------------- | :------------------ | ------------------------: | -------------------------: | --------------------------: |
| complex-lofi-mlp_4D_signed-pe-tanh       | signed                                               | tanh                |                      11.7 |                       15.6 |                        11.8 |
| complex-lofi-mlp_4D_signed-pe-leaky_relu | signed                                               | leaky relu          |                      2.34 |                       2.11 |                        1.98 |
| complex-lofi-mlp_4D-pe-tanh              | abs val                                              | tanh                |                        19 |                       20.1 |                        16.3 |
| complex-lofi-mlp_4D-pe-leaky_relu        | abs val                                              | leaky relu          |                      2.18 |                       2.72 |                        2.18 |
2. Recap of what we thought would happen with different mlp output: 
   By fixing the output layer to take the acceleration's absolute value, we avoid the behavior that we sometimes saw where acceleration would go to nearly 0 because acceleration direction vector was flipping so its components were crossing 0. We hoped and hypothesized that this fix would improve training in terms of better generalization with less data, faster convergence, more accurate acceleration predictions, etc. Recall that results sent yesterday reflected training on 100 complex orbits, and I wanted to see if the "fixed" output layer would result in better generalization capabilities when training on fewer orbits. 
   
   What we're actually seeing:
   Today, I have been training on 10 random complex orbits, and testing on separate validation datasets of 1, 10, and 100 complex orbits. I was seeing that training with a signed acceleration magnitude was actually resulting in both lower training losses and better generalization than with the absolute value of the magnitude, which was not we expected. A couple thoughts on this: 
	- In the case of training on the signed acceleration output pipeline for comparison, I inspected the output features for all of the training orbits. I never see the previously observed behavior where acceleration direction and magnitude randomly flips. This doesn't mean it can't/won't happen - I want to figure out in what cases it was originally occurring. It is a failure mode that we know does sometimes exist, it's just not showing up right now.
	- I noticed the loss curves when using absolute value of acceleration were not converging smoothly and showed chaotic looking behavior. I've concludedj that abs value isn't appropriate to use since it is nondifferentiable at 0. I updated the output layer to take softplus(acc) rather than abs(acc) for more stable gradients, and this has made training much smoother. Here is an example of acceleration error in the case where our length strategy is train on 10% then 100%, segment length is 5% of total orbit, activation function is leaky relu. Yellow uses abs(acc) in the output layer, blue uses the signed acceleration, and red uses softplus(acc). Signed acceleration learns the acceleration more accurately and generalizes better, but red is much smoother. Here is the average acceleration error for each case on three validation datasets:

| training scenario | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |     |
| :---------------- | ------------------------: | -------------------------: | --------------------------: | --- |
| mlp_4D_signed     |                      4.54 |                       6.26 |                        5.76 |     |
| mlp_4D_abs        |                      13.2 |                         10 |                        7.66 |     |
| mlp_4D_sigmoid    |                      11.8 |                         11 |                        9.63 |     |
	with the following config (only changing output layer between runs):
``` python
parameters:

  length_strategy:
                      [[[0.0,0.1],
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, 0.001]]
  steps_strategy: [[500, 5000]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 256
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D_accsoftplus
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  	```

Takeaways:
- There is more investigating to be done - I'm surprised that using the signed acceleration in the output layer results in the best model and best generalization for the given validation datasets, but I'm not convinced this is always the case since the aforementioned failure mode did not appear in these results.
- To that end, I wonder and have been testing if there are certain scenarios where the softplus(acc) is necessary. This boils down to: when do we see the flipping acceleration direction failure mode? Perhaps we more frequently see this failure mode with shorter segments, less data, certain loss functions or activation functions, certain data distributions, etc. 
- In general, acceleration error is still too high in any of these cases. I want to get this down. I showed yesterday that percent error + rmse loss helped, and I will investigate other changes further. 
## Follow up
- When we apply the model to validation orbits, should we be applying to an initial condition, propagating for one whole orbit and then calculating acceleration error based on this? Or should we be segmenting the validation orbits in some way as well?
- Why is leaky relu so much better than tanh?
- how do I prevent my hands from freezing off
- does length strat matter?
- does segmentation strat matter? will this make mlp_4D vs mlp_4D_signed matter?
- It might be useful to have a functionality where we can pull runs down, compare their configs and display to users which parameters are different
- what are the spikes in our loss when we're training for longer?
- when we train, we expect to get lower acceleration errors than what we are seeing. What improvements can we make?
	- batch size
	- output layer
	- input features
	- etc? 
- review codex answer
- Do mlp_4D_signed models struggle at periapsis in the same way that mlp_4D do?
## Notes to self:
- Make sure that if/when we put final activation back in, if we plot the output features then it should account for them going through this activation function. 

# December 2
## Datasets
### 1 simple
![[Pasted image 20251202121721.png|500]]
![[Pasted image 20251202121726.png]]

### 1 complex
![[Pasted image 20251202121714.png|500]]
![[Pasted image 20251202121738.png|1000]]
### 5 simple


### 5 complex
### 10 simple
### 10 complex
### 100 simple

### 100 complex
![[Pasted image 20251202121226.png]]
![[Pasted image 20251202121232.png]]

## Training
### complex-hifi-mlp_4D_signed
run_id: bl3987og (name different-aardvark-5734)
runtime: 4:42
config:
``` python
wandb:
  group: "complex-hifi-mlp_4D_signed"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:



  # EXHAUSTIVE LENGTH STRATEGY
  length_strategy:    [[
                        [0.0, 0.1],
                        [0.0, 0.2],
                        [0.0, 0.3],
                        [0.0, 0.4],
                        [0.0, 0.5],
                        [0.0, 0.6],
                        [0.0, 0.7],
                        [0.0, 0.8],
                        [0.0, 0.9],
                        [0.0, 1.0],]
                      ]

  lr_strategy: [[0.001, 0.001, 0.001,0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]]
  steps_strategy: [[500, 500, 500, 500, 500, 500, 500, 500, 500, 500]]
  segment_length_strategy: [[18,]]


  width: 64
  depth: 2
  train_val_split: 1
  batch_size: 64
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D_signed
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

## complex-hifi-mlp_4D
run_id: wkifs0zj (name denim-wave-5735)
runtime: 4:42

## complex-lofi-mlp_4D
run_id: kz4x0xl0 (name rose-resonance-5763)
runtime: 1:13

## complex-lofi-mlp_4D_peplusrmse
run_id: w6uiz2b4 (name vital-mountain-5737)
runtime: 1:25
config:
``` python
wandb:
  group: "complex-lofi-mlp_4D-peplusrmse"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_100_train"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[18,]]

  width: 64
  depth: 2
  train_val_split: 1.0
  batch_size: 64
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error"
  loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

| label              | wandb_group_id                 | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |     |
| :----------------- | :----------------------------- | ------------------------: | -------------------------: | --------------------------: | --- |
| HiFi MLP (pe)      | complex-hifi-mlp_4D            |                      5.49 |                       5.48 |                        4.71 |     |
| LoFi MLP (pe)      | complex-lofi-mlp_4D            |                      2.62 |                       3.28 |                        2.73 |     |
| LoFi MLP (pe+rmse) | complex-lofi-mlp_4D-peplusrmse |                      2.76 |                       2.72 |                        2.34 |     |

Metric: average acceleration error
Testing datasets in last three columns

| training config label                | Acceleration output - signed (bad) or abs val (good) | Total steps | Runtime (min:sec) | Loss                                                | complex_TBP_planar_1_test | complex_TBP_planar_10_test | complex_TBP_planar_100_test |
| :----------------------------------- | :--------------------------------------------------- | :---------- | :---------------- | :-------------------------------------------------- | ------------------------: | -------------------------: | --------------------------: |
| HiFi (conference config)             | signed                                               | 9500        | 10:02             | Percent error                                       |                      3.87 |                       3.57 |                        3.45 |
| HiFi (abs value acceleration output) | abs val                                              | 9500        | 10:21             | Percent error                                       |                      2.86 |                       3.08 |                        2.86 |
| LoFi                                 | signed                                               | 1000        | 1:08              | Percent error                                       |                      3.62 |                       3.30 |                        3.28 |
| LoFi                                 | signed                                               | 1000        | 1:15              | Percent error +  RMSE (normalized to similar scale) |                      2.56 |                       3.26 |                        3.60 |
| LoFi                                 | abs val                                              | 1000        | 1:18              | Percent error                                       |                      2.62 |                       3.28 |                        2.73 |
| LoFi                                 | abs val                                              | 1000        | 1:25              | Percent error +  RMSE (normalized to similar scale) |                      2.76 |                       2.72 |                        2.34 |



# November 26
Fixing commit history
# November 25:
- try fixing the train/val split back so that we are segmenting and then doing train/test split
- pipeline - length and segmentation strategy
- model - 
- training and testing on the same data is not good
	- perhaps we have explicit training and validation datasets
	- validation may be better
	- try different order of ops for split then segment and segment then split
- order of ops - exact same metrics, same pipeline
	- one orbit
	- we may have poor metrics
- set up code for additional metrics with validation sets
## training w longer segments
Differences:
- use percent error loss rather than percent error + rmse to make the model lighter
- segment orboits into 36 segments rather than 90
run_id: h8h0htoa
runtime: 1:09
config:
``` python
parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[2000]]
  segment_length_strategy: [[10,]]

  width: 32
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error_with_attraction"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  # output_layer: mlp_4D_activation
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple_hybrid
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```
![[Pasted image 20251125114415.png]]
# November 21
TODO:
- [ ] cleanish commit history
- [ ] document changes
- [ ] compare convergence for model a) before any chagnes were made, b) assuring force is attractive, c) 

# November 18
Breakthrough!! We realized that the feature layer that we were using, mlp_4D (and its variants), 
## baseline
``` python
wandb:
  group: "2BP-sensitivity"  # Change this to your desired group name

data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[3000]]
  segment_length_strategy: [[4,]]

  width: 32
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  output_layer: mlp_4D
  # output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```

run_id: z0v2rmhl
time to run: 1:29


![[Pasted image 20251118115157.png]]
![[Pasted image 20251118115124.png]]
![[Pasted image 20251118115208.png]]
![[Pasted image 20251118115228.png]]
![[Pasted image 20251118115234.png]]
![[Pasted image 20251118115254.png]]


## calculating mlp_4D using absolute value of acceleration 
# November 18
- Spent last week mostly making QoL changes to codebase, but still need to figure out what's going on with training
- We frequently see that when we change the method of training, the model will lag (or lead) the true dynamics

Debugging this behavior:
![[Pasted image 20251112102231.png]]
![[Pasted image 20251117222526.png]]
Why is acceleration magnitude differnt between the two above?

![[Pasted image 20251117222555.png]]
![[Pasted image 20251117222606.png]]
![[Pasted image 20251117222617.png]]

``` python 
self.mlp = eqx.nn.MLP(
            in_size=in_size,
            out_size=out_size,
            width_size=width,
            depth=depth,
            activation=getattr(jnn, config.parameters.activation),
            # final_activation=jnn.tanh,
            key=key,
        )
        ```

The model doesn't look converged - what if we train for longer?
## Enforcing unit vector for r
``` python
data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  length_strategy:
                      [[
                        [0.0, 1.0],
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 4
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  # activation: tanh
  activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  # feature_layer: sph_4D_rinv_vinv
  # output_layer: mlp_4D
  output_layer: mlp_4D_unit_scaled
  # output_layer: mlp_simple
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
![[Pasted image 20251117230413.png]]
![[Pasted image 20251117230421.png]]
![[Pasted image 20251117230431.png]]
![[Pasted image 20251117230354.png]]
![[Pasted image 20251117230458.png]]
![[Pasted image 20251117230448.png]]
Thoughts/questions:
- How does activation function affect things?
- final layer activation function necessary? 
- 


















# November 11

Goal: figure out why acceleration is dipping:
![[Pasted image 20251112102231.png]]
TODO:
- [ ] verify input and output features
- [ ] speed up training with features
- [ ] fix display of input and output features - why is it only happening for one orbit?
- [ ] clean up and commit code
- [ ] fix loss curves - training steps are not currently being plotted with the correct x axis

Why do we have __call__ and solve_with_feature_capture in NeuralODE class when they do basically the same thing? 
## Baseline
config:
``` python
parameters:

  length_strategy:
                      [[ 
                        [0.0, 1.0],                        
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 4
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  loss_fcn: "percent_error"

  activation: leaky_relu

  feature_layer: sph_4D_rinv_vel

  output_layer: mlp_4D

  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
```
7j3cjn5u

## Activation functions:
![[Pasted image 20251112102520.png]]
# November 10

What does the output layer do?
Order of operations:
$y = [x, y, z, v_x, v_y, v_z]$ stores position and velocity. We load in our training data and normalize it, so y is already normalized before being passed into the training loop.
Then, the ODE we're learning needs $dy/dt$ which contains both $dx/dt$ (the velocities) and $dv/dt$ (the accelerations). The feature layer transforms $y$ into a compact/intelligent representation of our choice; the MLP turns that into a latent vector; the output-layer function turns that latent vector (and possibly the original stat) into a 6-vector $[\dot{x}, \dot{y}, \dot{z}, \dot{v_x}, \dot{v_y}, \dot{v_z}\}$. Diffrax integrats that to then compute losses and gradients.

**The end goal is always a 6‑dimensional dy/dt, but you don’t have to predict it all in one shot. Each output-layer helper defines a structured way to turn the MLP’s latent vector (and sometimes the current state) into that same derivative:

- mlp_simple assumes the MLP already emits a 6‑vector, so the head just returns it.
- mlp_simple_hybrid says “the first three components (dx/dt) should just be the current velocity,” so the MLP only predicts the acceleration part and we concatenate [vx, vy, vz, ax, ay, az].
- mlp_4D variants force the MLP to output an acceleration magnitude plus a direction vector, and the head reconstructs [vx, vy, vz, ax, ay, az] by combining the state’s velocities with that structured acceleration.

All of them end up with the same final derivative, but they impose different inductive biases or parameterizations on how the accelerations are produced. Choosing one head over another just changes how much structure you bake into the model before handing dy/dt to Diffrax and the losses.

**

xactly. mlp_output is just the latent vector coming out of the shared MLP—it’s the head’s job to interpret that latent into the actual state derivative. Depending on which output-layer helper you choose, that latent might encode:

- the entire 6‑vector derivative (mlp_simple),
- only the acceleration part (then the head splices in velocities from the state), or
- something more structured like [acc_magnitude, direction_vector] that the head expands back into accelerations.

So mlp_output itself isn’t the physical state; it’s whatever intermediate representation you’ve designed the head to work with before producing the final [dx/dt, dv/dt].
- Data prep happens in scripts/training/train.py: raw orbits are normalized (Normalization2BP), split, and batched. Each training batch supplies time grids ti, initial states yi[:,0,:], and masks.
    
- The model (neuralODE/neuralODE.py (lines 44-205)) is an Equinox module: a feature layer transforms each state y into a feature vector, a shared MLP maps features to an output vector, and an output-layer head (e.g., mlp_4D in neuralODE/output_layers.py (lines 12-66)) turns that vector into the 6‑dimensional state derivative. This derivative is the RHS fed to Diffrax’s diffeqsolve.
    
- Forward pass: for each trajectory in a batch, jax.vmap(model, in_axes=(0,0)) (neuralODE/losses.py (lines 6-22)) integrates the ODE across the time grid, producing predicted trajectories y_pred.
    
- Losses (neuralODE/losses.py (lines 23-209)) compare y_pred to the ground truth yi, typically via percent error or MSE variants. These losses may also add regularizers (L1/L2) or physics terms (energy drift).
    
- Backprop: Optax optimizers take loss(model, batch); JAX automatically differentiates through the Diffrax solver (Tsit5 with adjoint handled by Diffrax). Gradients flow from loss → predictions → solver states → model outputs → MLP weights/features. The optimizer updates the MLP parameters (and any scalars) each step.
    
- Metrics/visualizers: after training, helper routines capture feature-layer values, output-layer intermediates, and accelerations via solve_with_feature_capture, plot them (neuralODE/visualizers/feature_inputs.py), and log to WandB for diagnostics.

- [x] total loss, percent error, rms on the same plot
	- [ ] hope that we get tradeoff between loss functions - if not this may indicate that we are in a local minimum
	- [x] replace staircase representation in val loss with interpolated val loss
	- [x] simplify problem
	- [ ] John suspects activation function will help a lot - it looks like there is a critical radius where we go from being attracted to flying away. why do we go from attractor to repulsive force. we expect to see conical section still. 
	- [ ] accelerations should be negative in the radial direction - we could design to say a certain number must be negative. dot product between accel and position vector should be more than 90 degrees out of phase. show the direction of the acceleration vector. 

order of ops:
- [x]  simplify orbits
- change activation gelu, relu, leaky relu
- plot outputs for actual acceleration mag and direction
	- maybe new loss constraint - penalize if force isn't attractive
- expect to find a loss function that is suitable for different problems
- shouldn't really need to weight mse since everythign is already nondimensionalized

look at inputs as a function of time
John is concerned that position is constrained -1 to 1
plotting states as a function of time - true vs discrepant orbit 
input - velocity magnitude and an angle for that. or replace the velocity components completely.

QoL improvement - I want loss curves to save by default after training. This isn't happening right now . FIXED

I want to observe if acceleration is force is attractive and penalize otherwise. 
# November 7
Let's get a baseline without any learning curriculums for troubleshooting. `complex_TBP_planar_1`:
![[Pasted image 20251107160541.png|500]]
![[Pasted image 20251107160547.png|1000]]
complex_TBP_planar_4:
![[Pasted image 20251107161440.png|500]]
![[Pasted image 20251107161434.png|1000]]

## v1 - baseline
We'll start by training on complex_TBP_planar_4 to begin, with the following validation orbit:
![[Pasted image 20251107161725.png]]

The baseline config does not include any training curriculum:
``` python
data:
  dataset_name : "complex_TBP_planar_4"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],                        
                      ]]
  lr_strategy: [[0.001, ]]
  steps_strategy: [[1000, ]]
  segment_length_strategy: [[4,]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001```
losses:


![[Pasted image 20251107165931.png]]

![[Pasted image 20251107170022.png]]
# November 5

Debugging heat map.

Let's recall our baseline models that we are using for comparison and building from.
currently, we are testing on `complex_TBP_planar` which has 100 orbits. This might be too much for now, so let's reduce to 1, 4, 16 orbits (this is where our original heatmap issue came from anyway)

Recall these are the datasets we are using:
![[Pasted image 20251107095101.png|500]]
![[Pasted image 20251107095245.png|1000]]

Let's start by training on the dataset with 16 orbits. 
## v1 - segmentation strategy, no length strat, percent errror loss

config:
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 1.111%, 5%, 6.666% 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
losses:
![[Pasted image 20251107102932.png]]
![[Pasted image 20251107102946.png]]

randomly selected reference orbit:
![[Pasted image 20251107104833.png]]
![[Pasted image 20251107104900.png]]

- As we increase our segment lengh, the acceleration error does initially improve but then spikes. This could indicate that the segment length in phase 3 is too long, or we have a earning rate is too high because we see that our loss and acceleration error just oscillate
- Before we declare that the length is too long, let's try lowering the learning rate for the final phase. We should probably implement a learning rate decay schedule

## v2 - segmentation strategy, no length strat, percent error loss, lower lr for final phase

Difference from previous - lower the learning rate in the last phase
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001```

loss:
![[Pasted image 20251107111033.png]]
![[Pasted image 20251107111043.png]]
![[Pasted image 20251107110612.png]]

- Lowering the learning rate did help, but it looks like segment length of 24 is still too long for this case
- Let's try increasing the segment length

## v3 - segmentation strategy, no length strat, percent error loss
Difference from previous: decrease the last segment length
``` python
parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
loss:
![[Pasted image 20251107114322.png]]
![[Pasted image 20251107114332.png]]
![[Pasted image 20251107114051.png]]
- This didn't really help but we're not converged - let's increase training steps

## v4
Difference from last - increase steps_strat for phase 1
``` python
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

![[Pasted image 20251107121722.png]]
![[Pasted image 20251107121834.png]]
![[Pasted image 20251107115147.png]]
Interestingly, we do actually track a little better in the beginning with the longest segmentation ratio but then our errors explode. Let's try implementing a loss function that also penalizes mse.

## v5 - combined loss
Difference from last - we are using a loss combining percent error and mse. 
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200 ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error"
  loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

The loss funciton now combines percent error and mse. mse is scaled so it is on a similar scale to percent error, and incliudes a weighting factor for further control:
``` python 
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=1.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse
    ```
![[Pasted image 20251107123714.png]]
![[Pasted image 20251107123725.png]]
![[Pasted image 20251107123808.png]]
![[Pasted image 20251107123245.png]]
- This is tracking a bit worse, but from comparing the magnitude of the percent error vs the actual tracked loss, the mse is contributing very little. 
- It also looks like our model may not be sufficiently complex
	- ETA: this was not true - if we change the width and depth to 32 and 3 respectiely, our acceleration error is much worse (this is in run astral-wave 5159): ![[Pasted image 20251107125020.png]]
	![[Pasted image 20251107125037.png]]
	![[Pasted image 20251107125052.png]]
Let's try weighting the mse more heavily. 
Note - the mse weight should be a parameter in the config. I'll fix this later


## v6 - combined loss w/increased weight
Difference: same as before, but mse weight = 50
![[Pasted image 20251107125856.png]]
![[Pasted image 20251107125914.png]]
![[Pasted image 20251107130500.png]]

![[Pasted image 20251107125708.png]]


TODO:
- [ ] total loss, percent error, rms on the same plot
	- [ ] hope that we get tradeoff between loss functions - if not this may indicate that we are in a local minimum
	- [ ] replace staircase representation in val loss with interpolated val loss
	- [ ] test behavior with 1 elliptic orbit
	- [ ] simplify problem
	- [ ] John suspects activation function will help a lot - it looks like there is a critical radius where we go from being attracted to flying away. why do we go from attractor to repulsive force. we expect to see conical section still. 
	- [ ] accelerations should be negative in the radial direction - we could design to say a certain number must be negative. dot product between accel and position vector should be more than 90 degrees out of phase. show the direction of the acceleration vector. 

order of ops:
- simplify orbits
- change activation gelu, relu, leaky relu
- plot outputs for actual acceleration mag and direction
	- maybe new loss constraint - penalize if force isn't attractive
- expect to find a loss function that is suitable for different problems
- shouldn't really need to weight mse since everythign is already nondimensionalized

look at inputts as a function of time
John is concerned that position is contrained -1 to 1
plotting states as a function of time - true vs discrepant orbit 
input - velocity magnitude and an angle for that. or replace the velocity components completely.















##  OLD v3 - segmentation strategy, no length strat, combined loss, mse_weight = 10 (this was usi)
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error_plus_nmse"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

where the loss `percent_error_plus_nmse` is:
``` python
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=10.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse```

![[Pasted image 20251107095852.png]]
![[Pasted image 20251107095907.png]]
![[Pasted image 20251107100032.png]]



- [ ] we want to implement save best model, and ideally if we could do it by phase that would be even better
- [ ] what is a better way to test? right now we are visualizing each phase against a random validation trajectory
- [ ] add mse weight to the config
# November 5
Sick last couple days, trying to make up time.

To Do:
- [ ] better understanding of segmentation training curriculum
- [ ] 
We got the segmentation curriculum working in train.py and neuralODE.py. Transferred analogous changes to sweep.py and added some padding (but need to update to add padding for all strategies - I think I have this stashed in code or obsidian somewhere?)

On my quest to get my heat map fixed:
First, let's record changes that were made in order to plot different phases of training both for record keeping and to triple check that the behavior we are seeing in training is correct. 


testing new loss function:

# October 31
Fixing segment length strategy:
Let's move the segmenting portion of the data processing to within train_model so that we can keep the training script relatively clean. we can still load the total dataset into the main script and then 

We found that it is important to segment each training orbit into smaller segments when training our neuralODE. Otherwise, we were seeing vanishing gradients which we hypothesize is in part because the time horizon for integration is too long when we do autodiff. We now want to investigate if a training curriculum in regards to segmentation strategy would be useful, e.g. will we see improved training if we train on shorter segments then increase the lenght of these segments (though there is likely a point of diminishing returns). 

I want to be able to implement this in our scripts. We had a first pass but this put the strategy implementation within the main script, which isn't really in line with how we handle curriculum training in the case of length_strategy. Can we pass the config and original dataset to train_model, and do the segmentation within train_model based on our current phase of training or is that not the best idea? I still want to be able to log to wandb appropriately. For context, the git stash contains the previously recommended method of doing all of the dasta segmentation within the main, while I would prefer this happens within neuralODE.py l.ike the rest of our strategy work. 

Training is working as expected now in `train.py`. Let's double check `sweep.py`. Also check that metadata that is uploaded is correct now that we have a new method of training. I'll do a quick spotcheck.

I want to be able to visualize how each phase of the model training tracks an orbit. I think a good method to do this would be to load the dataset, average the orbital elements of each orbit in the dataset, and apply the model at these averaged initial conditions (but apply the model at the end of each phase of training if possible). Visualize the true orbit vs the model predicted orbit in a manner similar to Integration Visualizer at each phase, and plot acceleration statistics.
# October 29
Finishing conda-pack and unpack
# October 28
I am getting Zaratan set up so I can run large numbvers of training scripts.

Ideas for improving training:
- try new coordinates
- different loss functions
- different input and output layers
- length strategy
- segmentation curriculum learning

## Zaratan
`module load:`
- `module load <modulename>` temporarily modifies your environment so you can use specific software packages or versions without needing admin rights
	- ex. `module load python` loads python 3.10.10
- we need to use `module load` each time we start a new session unless it is automated in .bashrc
- university recommends that `module load` commands do not go in .bashrc, rather just put them in the submitted bash script

Slurm:
- `squeue --me` shows queued jobs in pending state
- `sbalance` shows account balance (I think this is shared across the lab)
- each core on Zaratan is 4GB mem
- `#SBATCH --ntasks=<n>` specifies the number of parallel tasks (processes) to launch - you would use this if running multiple instances of your program (so this would be good for me)
- `#SBATCH --cpus-per-task=<n>` is the number of cpu cores assigned to each task. Use this for multithreaded programs (like numpy) that need multiple cores per process

Getting my conda env set up:
- I have project dependencies - how do I know if these are installed on zaratan and how can I install things that are needed? I think that's the point of the conda env - I install in there in my active conda env
- I'm pretty sure that I ended up needing to pip install a few more things - how can I check curiosity to see what I have?
- am I able to change branches when I run this stuff?
-  
- can I pip install my project? I think I will have to pull the git repo into my scratch directory? do I pip install my project in my conda env?
- steps I think I will take:
	- git clone
	- pip install project ?
	- I'm also going to need to install mldsml and switch to the neuralODE-dev branch
	- is it necessary to pip install things from the pyproject.toml when it may be taken care of by just pip installing the project?
	- am I actually able to use wandb to pull datasets down from hpc? wandb on zaratan isn't communicating online but I imagine it can still pull it?
- how do I check the available memory in my scratch directory? I think I might have to store datasets on zaratan directly
- I'm going to have to pull files down from wandb. Is there a way I can pull them down and save them off locally so that we can use datasets that are already created?
- there are data transfer nodes on zaratan - how do I access them?

I am training a neuralODE. I have a git repo with many files, folders, etc. I have config.yaml files that are read in to the main training script, and these config files contain different parameters (like model size, loss function, etc). I want to run many different instances of the training script with different parameters based on these config files, so I am trying to get the code set up to run on my university's HPC. I am creating a conda environmnt to install all required dependencies. I also use another git repository central to my lab which I will have to pull down and pip install. I am looking for advice on how to get everything up and running. A few questions:
- can I pip install the repositories into the conda environment? will the dependencies also be installed?
- am I able to switch branches when I am running things on HPC? I need to be on a different branch of my lab repository
- will pip installing these projects automatically install the dependencies?

do NOT run jobs out of home directory
home directory is the only place where files are backed up - scratch and SHELL are not

```
source ~/scratch/miniforge3/etc/profile.d/conda.sh

conda activate env_name

```
# October 21

I am training neural ODEs to learn 2BP dynamics, and for simplicity I am beginning with the planar case. Through my research, I found that it is very important to each orbit in the training dataset into smaller trajectories because otherwise we see vanishing gradients (hypothesized that when we use the adjoint sensitivity method to compute the gradients, the integrand is overly stable such that when we integrate over time horizons that are too long, we get vanishing gradients). We still think length strategy is important, which refers to training on a portion of the time series of the data to warm start the model and decrease computation time, but in this plot we are just training on the entire time series. 

In this heat map, we are showing the effects of training on different numbers of randomly generated orbits and the effects of segmenting those training orbits into different numbers of segments. When we generate random training data, we set a range for each orbital element and then randomly select from there so our orbits range from LEO to GEO, circular to as eccentric as possible without intersecting the earth. The segmentation ratio indicates what percent of the total trajectory each segment makes up (e.g. 5% means that we segmented the orbit into 20 trajectories, each being 5% of the total orbit). We then test the model on datasets containing different numbers of randomly generated orbital initial conditions, apply the model to the random initial conditions, and find the percent error in the true vs model predicted acceleration averaged over all timesteps and orbits in a given test dataset.

The segmentation trends are generally as expected - the smaller the segment and shorter the time horizon for integration, the lower the error when applying the model to a test dataset. However, we don't see the expected monotonic trend that increasing number of training orbits decreases model error. 

For now, we are focusing on getting the expected smooth gradient in the bottom 3x3 (or 4x4 if necessary) grid of the heat map. Some ideas we've had:
1) to trouble shoot, try to simplify by first testing on ciruclar orbits with different sma. However, this might not be a good idea because when we learn 2BP dynamics we are basically learning the 1/r^2 term, so circular might actually be harder to learn? for context, this is our input layer:
``` python

def sph_4D_rinv_vel(y):
    pos = y[:3]
    radius = jnp.sqrt(jnp.sum(pos**2))
    return jnp.concatenate(
        [
            jnp.array(
                [
                    1 / radius,
                    pos[0] / radius,
                    pos[1] / radius,
                    pos[2] / radius,
                    *y[3:],
                ],
            ),
        ],
    )
```

and this is our output layer:
``` python
def mlp_4D(mlp_output, state, scalar=1.0):
    r_mag = mlp_output[0:1]
    r_dir = mlp_output[1:4]
    acc_pred = r_mag * r_dir
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
2) should we try curriculum training with segmentation strategy? we get better results when we start with small segments, but the loss curves are quite noisy (we should also test different learning rate). maybe train first on a smaller segmentation strategy and then increase it?
3) are our input and output layers sufficient?
4) are the other parameters sufficient?
5) We currently start with a random true anomaly at the first timestep of each orbit (before segmentation) - is this necessary?
---

Let's test on a few different datasets:
**sensitivity-2D-baseline-v1:**
This tests on randomly generated orbits with the following possible ranges/params:
- SMA = \[R_Earth + 200km, R_GEO\] (where GEO is 42164 km)
- e = \[0, 0.99\] (in reality we check for intersections, so the max e is the largest eccentricity without intersecting the earth)
- planar (i = 0, RAAN and argument of periapsis undefined)
![[orbital_elements_hist_baseline 1.png]]
![[multi_dataset_xy_baseline 1.pdf]]

![[heatmap_data_sensitivity_baseline.png]]

![[W&B Chart 10_21_2025, 11_13_27 AM.png]]

![[W&B Chart 10_21_2025, 11_14_09 AM.png]]
**sensitivity-2D-fixednu0-v1**
let's generate datasets using the same random initial conditions but now nu0 = 0 for every dataset. naming scheme will be complex_TBP_planar_fixnu0_{num_orbits}

![[multi_dataset_xy_fixnu0 1.pdf]]
![[orbital_elements_hist_fixnu0.png]]
Now let us train the model
Testing on the baseline datasets with not fixed nu0:

![[heatmap_data_sensitivity_fixnu0.png]]

Testing on fixed nu0:
![[heatmap_data_sensitivity_fixnu0_testfixnu0.png]]


![[W&B Chart 10_21_2025, 11_12_54 AM.png]]
![[W&B Chart 10_21_2025, 11_12_25 AM.png]]

**sensitivity-2D-circular-v1**
This tests on circular orbits with random SMA:
![[multi_dataset_xy_circular.pdf]]

![[orbital_elements_hist_circular.png]]
Now let's train:
![[W&B Chart 10_21_2025, 11_32_15 AM.png]]

![[W&B Chart 10_21_2025, 11_32_57 AM.png]]

Testing on original datasets:
![[heatmap_data_sensitivity_circular_testbaseline.png]]
Testing on circular (training) datasets:
![[heatmap_data_sensitivity_circular.png]]























































# October 16
## Recap
We are working on 2D sensitivity analysis. Preliminary results were surprising - we were expecting to see a monotonic trend that as the number of training orbits increases, the error decreases and that as the segment ratio decreases, the error decreases. The latter is true but the former is not:
![[Pasted image 20251016122314.png]]
![[Pasted image 20251016122305.png]]
We are trying to figure out why this is happening, and want to focus on getting a nice gradient while just looking at the lower left 3x3 or 4x4 grid. 
We also noticed that the loss curves associated with this training are very noisy:
![[Pasted image 20251016122832.png]]
![[Pasted image 20251016122946.png]]
This is probably due to a variety of parameters and hyperparameters. Learning rate may be too high, segmentation strategy may put us in a local minimum that we can't get out of (maybe? double check notes from meeting), might need a training curriculum, etc. Try to get the bottom 3x3 or 4x4 of the heat map behaving expected by testing the interactions of these parameters, implementing training curriculum, etc.

## Sensitivity Analysis
Getting to the bottom of the issue described above...
The issue that we are currently investigating is that as we increase the number of training orbits, our results are not monotonically improving.

### Training data
Let's look at what our training datasets were for the first 4x4 grid:

**TBP_complex_planar_1:**
\[a, e, i, omega, w, nu\] = \[3.5359730e+04 3.7370604e-01 0.0000000e+00           nan           nan
 5.6238999e+00]\
 SMA = 35359.73 km (GEO is ~42 km)
 e = .3737
 nu0 = 322.23 deg
 ![[Pasted image 20251017124525.png|200]]
 
TBP_complex_planar_4:
![[Pasted image 20251017125450.png]]
Note to self - verify that I apply models to the test datasets
 
# October 7
## Recap
- I've begun work on 2D sensitivity studies but need to get more experiments and visualizations up and running. We need to start using the "num_trajs" parameter again for analysis.
- Finished the ASTRA poster yesterday. I made some modifications to the 2BP vector field visualization and experiment that should be documented. 
- I'm having some memory issues when I work locally, but I am having some issues with Docker and installing packages using `pip install -e .`
- I noticed that change $\nu_0$ for the test orbit after training 3D model changes the results largely, which is a bit perplexing since we also segment data. I want to investigate this. 

## Goals
- [x] Fix Docker issues
- [ ] Clean and document vis and experiment from yesterday
- [ ] 2D sensitivity studies. At a min, study num_traj vs segment_length
- [x] quick 3D study so we can talk about it in meeting
- [ ] there is one more stash from the conference I have to go through and apply/document
- [ ] prevent wandb files, artifacts, etc. from being saved locally
## Sensitivity studies
Step 1 is being able to access the parameters we want to study! For now, these include:
- [ ] number of training orbits
- [ ] segmentation strategy
- [ ] length strategy
Segmentation strategy and length strategy are already easily accessible. We also previously utilized `num_trajs` param, but this has basically been phased out in favor of training using the explicit names of datasets defined in the config. So, we can either consistently name datasets and put the num_trajs as a variable within the name (e.g. "simple_TBP_planar_5"), we can dynamically find the number of trajectories by loading the data, we can include another num_trajs param to weep over (but we would have to make sure that we have it properly corresponding to the correct dataset), or we can add a parameter to the dataset itself indicating the number of trajectories. The last option is probably our best bet since reloading data unnecessarily is expensive. 

Basically, this just means that when we train the model we should record how many orbits the model was trained on (taking into account train_test_split). We already have to load the data for training of course, so at that point we can record the number of training orbits to wandb.

I updated the training script and model save function so that we can pass optional additional arguments to log to the model. Currently, I added segment_ratio so we have direct access to the percentage each segment is of the total trajectory, as well as num_total_orbits and num_train_orbits so we don't have to do any additional data manipulation post model training.

Let's run a quick train script and make sure everything is working as expected.

Note - training is taking a very long time to run locally through docker because a lot of space isbeing used on saving artifacts, models, etc. We don't want to do this since everything is just uploaded to wandb and we can pull it down later. 



# October 6
## Recap
Working on sensitivity studies for 2D scenarios (segmentation, amount of training day, length strategy in particular)
## Goals
- [x] Finish ASTRA poster
	- [x] vector field diff plot overlaid with training orbits for a quick glance of how the training data amount impacts training
- [x] Figure out why 3D training isn't working well on test orbit (kinda done)
## 3D training
Last week, I began training on 3D orbits (as opposed to the planar ones we used for the conference). The segment length is 10, i.e. the trajectory is split into 36 segments before training. The loss curves are converging to acceptable values:
![[Pasted image 20251006110049.png]]
I did a sanity chck by testing on a random orbit that fell within the bounds of the training data and got very bad results:
![[Pasted image 20251006110119.png]]
During group meeting, Kruti asked if I had tried differ initial conditions. I had tried altering the shape of the orbit and got similarly bad results, but I had not altered the initial true anomaly.  When I change nu0 from 0 to 20 degrees, I get much better results:
![[Pasted image 20251006110526.png]]
![[Pasted image 20251006110545.png]]


What does this indicate? When we generate data, we take a random initial true anomaly from 0 to 360 degrees. The other orbital elements are randomized according to our desired dataset. When we load the data in, we segment the trajectory. Because of this segmentation, I am surprised that the initial true anomaly would have such a large impact since the model is functionally exposed to *many* different initial true anomalies through segmentation. 
This leads me to wonder: 
1) What does the full predicted vector field look like? How does this change as a function of randomizing vs not randomizing the initial true anomaly (e.g. will results be significantly different if we just always have an initial true anomaly = 0)? If we keep nu0 of the full trajectory randomized, how does the amount of training data affect the model accuracy? We would expect that increased exposure to more ICs, i.e. more training data, would improve accuracy.
2) How important is segmentation in 3D? We expect that it will have a larger effect than in 2D due to the increased dimensionality of the problem - to what extent is this true?

experiment idea for 2d: plot the residual of the vector field against the training orbits

## ASTRA Poster
I want to create the following visualization:
Train on 10 random 2D orbits (complex case). Use all orbits for training, i.e. train/val split = 1. Plot a vector field with the residual of the true versus predicted acceleration, overlayed on top of the training orbits.
- [ ] Generate training data
- [ ] Visualize
- [ ] Update yaml
- [ ] Update train script (use correct yaml)
- [ ] Train
- [ ] Visualize


