# October 7
## Recap
- I've begun work on 2D sensitivity studies but need to get more experiments and visualizations up and running. We need to start using the "num_trajs" parameter again for analysis.
- Finished the ASTRA poster yesterday. I made some modifications to the 2BP vector field visualization and experiment that should be documented. 
- I'm having some memory issues when I work locally, but I am having some issues with Docker and installing packages using `pip install -e .`
- I noticed that change $\nu_0$ for the test orbit after training 3D model changes the results largely, which is a bit perplexing since we also segment data
- 

# October 6
## Recap
Working on sensitivity studies for 2D scenarios (segmentation, amount of training day, length strategy in particular)
## Goals
- [x] Finish ASTRA poster
	- [x] vector field diff plot overlaid with training orbits for a quick glance of how the training data amount impacts training
- [x] Figure out why 3D training isn't working well on test orbit (kinda done)
## 3D training
Last week, I began training on 3D orbits (as opposed to the planar ones we used for the conference). The segment length is 10, i.e. the trajectory is split into 36 segments before training. The loss curves are converging to acceptable values:
![[Pasted image 20251006110049.png]]
I did a sanity chck by testing on a random orbit that fell within the bounds of the training data and got very bad results:
![[Pasted image 20251006110119.png]]
During group meeting, Kruti asked if I had tried differ initial conditions. I had tried altering the shape of the orbit and got similarly bad results, but I had not altered the initial true anomaly.  When I change nu0 from 0 to 20 degrees, I get much better results:
![[Pasted image 20251006110526.png]]
![[Pasted image 20251006110545.png]]


What does this indicate? When we generate data, we take a random initial true anomaly from 0 to 360 degrees. The other orbital elements are randomized according to our desired dataset. When we load the data in, we segment the trajectory. Because of this segmentation, I am surprised that the initial true anomaly would have such a large impact since the model is functionally exposed to *many* different initial true anomalies through segmentation. 
This leads me to wonder: 
1) What does the full predicted vector field look like? How does this change as a function of randomizing vs not randomizing the initial true anomaly (e.g. will results be significantly different if we just always have an initial true anomaly = 0)? If we keep nu0 of the full trajectory randomized, how does the amount of training data affect the model accuracy? We would expect that increased exposure to more ICs, i.e. more training data, would improve accuracy.
2) How important is segmentation in 3D? We expect that it will have a larger effect than in 2D due to the increased dimensionality of the problem - to what extent is this true?

experiment idea for 2d: plot the residual of the vector field against the training orbits

## ASTRA Poster
I want to create the following visualization:
Train on 10 random 2D orbits (complex case). Use all orbits for training, i.e. train/val split = 1. Plot a vector field with the residual of the true versus predicted acceleration, overlayed on top of the training orbits.
- [ ] Generate training data
- [ ] Visualize
- [ ] Update yaml
- [ ] Update train script (use correct yaml)
- [ ] Train
- [ ] Visualize


