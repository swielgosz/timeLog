
Debugging heat map.

Let's recall our baseline models that we are using for comparison and building from.
currently, we are testing on `complex_TBP_planar` which has 100 orbits. This might be too much for now, so let's reduce to 1, 4, 16 orbits (this is where our original heatmap issue came from anyway)

Recall these are the datasets we are using:
![[Pasted image 20251107095101.png|500]]
![[Pasted image 20251107095245.png|1000]]

Let's start by training on the dataset with 16 orbits. 
## v1 - segmentation strategy, no length strat, percent errror loss

config:
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 1.111%, 5%, 6.666% 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
losses:
![[Pasted image 20251107102932.png]]
![[Pasted image 20251107102946.png]]

randomly selected reference orbit:
![[Pasted image 20251107104833.png]]
![[Pasted image 20251107104900.png]]

- As we increase our segment lengh, the acceleration error does initially improve but then spikes. This could indicate that the segment length in phase 3 is too long, or we have a earning rate is too high because we see that our loss and acceleration error just oscillate
- Before we declare that the length is too long, let's try lowering the learning rate for the final phase. We should probably implement a learning rate decay schedule

## v2 - segmentation strategy, no length strat, percent error loss, lower lr for final phase

Difference from previous - lower the learning rate in the last phase
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001```

loss:
![[Pasted image 20251107111033.png]]
![[Pasted image 20251107111043.png]]
![[Pasted image 20251107110612.png]]

- Lowering the learning rate did help, but it looks like segment length of 24 is still too long for this case
- Let's try increasing the segment length

## v3 - segmentation strategy, no length strat, percent error loss
Difference from previous: decrease the last segment length
``` python
parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```
loss:
![[Pasted image 20251107114322.png]]
![[Pasted image 20251107114332.png]]
![[Pasted image 20251107114051.png]]
- This didn't really help but we're not converged - let's increase training steps

## v4
Difference from last - increase steps_strat for phase 1
``` python
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200, ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  loss_fcn: "percent_error"
  # loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

![[Pasted image 20251107121722.png]]
![[Pasted image 20251107121834.png]]
![[Pasted image 20251107115147.png]]
Interestingly, we do actually track a little better in the beginning with the longest segmentation ratio but then our errors explode. Let's try implementing a loss function that also penalizes mse.

## v5 - combined loss
Difference from last - we are using a loss combining percent error and mse. 
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001, 0.001, 0.0001]]
  steps_strategy: [[1000, 200, 200 ]]
  segment_length_strategy: [[4, 18, 20, ]]

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1

  # loss_fcn: "mean_squared_error"
  # loss_fcn: "percent_error"
  loss_fcn: "percent_error_plus_nmse"

  activation: tanh
  # activation: leaky_relu
  # activation: elu

  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

The loss funciton now combines percent error and mse. mse is scaled so it is on a similar scale to percent error, and incliudes a weighting factor for further control:
``` python 
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=1.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse
    ```
![[Pasted image 20251107123714.png]]
![[Pasted image 20251107123725.png]]
![[Pasted image 20251107123808.png]]
![[Pasted image 20251107123245.png]]
- This is tracking a bit worse, but from comparing the magnitude of the percent error vs the actual tracked loss, the mse is contributing very little. 
- It also looks like our model may not be sufficiently complex
	- ETA: this was not true - if we change the width and depth to 32 and 3 respectiely, our acceleration error is much worse (this is in run astral-wave 5159): ![[Pasted image 20251107125020.png]]
	![[Pasted image 20251107125037.png]]
	![[Pasted image 20251107125052.png]]
Let's try weighting the mse more heavily. 
Note - the mse weight should be a parameter in the config. I'll fix this later


## v6 - combined loss w/increased weight
Difference: same as before, but mse weight = 50
![[Pasted image 20251107125856.png]]
![[Pasted image 20251107125914.png]]
![[Pasted image 20251107125926.png]]

![[Pasted image 20251107125708.png]]

















##  OLD v3 - segmentation strategy, no length strat, combined loss, mse_weight = 10 (this was usi)
``` python
data:
  dataset_name : "complex_TBP_planar_16"
  problem: '2BP'

parameters:

  ## MINIMAL LENGTH STRATEGY
  length_strategy:
                      [[ 
                        [0.0, 1.0],
                        [0.0, 1.0],
                        [0.0, 1.0],
                        
                      ]]
  lr_strategy: [[0.001]]
  steps_strategy: [[200, 200, 200, ]]
  segment_length_strategy: [[4, 18, 24, ]] # corresponds to 

  width: 16
  depth: 2
  train_val_split: 0.8
  batch_size: 32
  num_trajs: -1


  loss_fcn: "percent_error_plus_nmse"

  activation: tanh


  feature_layer: sph_4D_rinv_vel
  output_layer: mlp_4D
  planar_constraint: true

  rtol: 0.000001
  atol: 0.00000001
  ```

where the loss `percent_error_plus_nmse` is:
``` python
def percent_error_plus_nmse_loss(
    model,
    ti,
    yi,
    mask_i,
    mse_weight=10.0,
    eps=1e-8,
):
    """Blend percent error with a normalized MSE term expressed as percent."""
    y_pred = get_y_pred(model, ti, yi, mask_i)
    y_true = yi[:, 1:, :]
    y_pred = y_pred[:, 1:, :]

    threshold = 1e-8
    true_norm = jnp.linalg.norm(y_true, axis=-1)
    safe_denominator = jnp.where(true_norm < threshold, threshold, true_norm)
    mpe = jnp.linalg.norm((y_true - y_pred), axis=-1) / safe_denominator * 100
    mpe_mean = jnp.nanmean(mpe)

    mse = jnp.nanmean((y_true - y_pred) ** 2)
    ref_power = jnp.nanmean(y_true**2) + eps
    normalized_rmse = jnp.sqrt(mse / ref_power + eps) * 100.0

    return mpe_mean + mse_weight * normalized_rmse```

![[Pasted image 20251107095852.png]]
![[Pasted image 20251107095907.png]]
![[Pasted image 20251107100032.png]]



- [ ] we want to implement save best model, and ideally if we could do it by phase that would be even better
- [ ] what is a better way to test? right now we are visualizing each phase against a random validation trajectory
- [ ] add mse weight to the config
# November 5
Sick last couple days, trying to make up time.

To Do:
- [ ] better understanding of segmentation training curriculum
- [ ] 
We got the segmentation curriculum working in train.py and neuralODE.py. Transferred analogous changes to sweep.py and added some padding (but need to update to add padding for all strategies - I think I have this stashed in code or obsidian somewhere?)

On my quest to get my heat map fixed:
First, let's record changes that were made in order to plot different phases of training both for record keeping and to triple check that the behavior we are seeing in training is correct. 


testing new loss function:

# October 31
Fixing segment length strategy:
Let's move the segmenting portion of the data processing to within train_model so that we can keep the training script relatively clean. we can still load the total dataset into the main script and then 

We found that it is important to segment each training orbit into smaller segments when training our neuralODE. Otherwise, we were seeing vanishing gradients which we hypothesize is in part because the time horizon for integration is too long when we do autodiff. We now want to investigate if a training curriculum in regards to segmentation strategy would be useful, e.g. will we see improved training if we train on shorter segments then increase the lenght of these segments (though there is likely a point of diminishing returns). 

I want to be able to implement this in our scripts. We had a first pass but this put the strategy implementation within the main script, which isn't really in line with how we handle curriculum training in the case of length_strategy. Can we pass the config and original dataset to train_model, and do the segmentation within train_model based on our current phase of training or is that not the best idea? I still want to be able to log to wandb appropriately. For context, the git stash contains the previously recommended method of doing all of the dasta segmentation within the main, while I would prefer this happens within neuralODE.py l.ike the rest of our strategy work. 

Training is working as expected now in `train.py`. Let's double check `sweep.py`. Also check that metadata that is uploaded is correct now that we have a new method of training. I'll do a quick spotcheck.

I want to be able to visualize how each phase of the model training tracks an orbit. I think a good method to do this would be to load the dataset, average the orbital elements of each orbit in the dataset, and apply the model at these averaged initial conditions (but apply the model at the end of each phase of training if possible). Visualize the true orbit vs the model predicted orbit in a manner similar to Integration Visualizer at each phase, and plot acceleration statistics.
# October 29
Finishing conda-pack and unpack
# October 28
I am getting Zaratan set up so I can run large numbvers of training scripts.

Ideas for improving training:
- try new coordinates
- different loss functions
- different input and output layers
- length strategy
- segmentation curriculum learning

## Zaratan
`module load:`
- `module load <modulename>` temporarily modifies your environment so you can use specific software packages or versions without needing admin rights
	- ex. `module load python` loads python 3.10.10
- we need to use `module load` each time we start a new session unless it is automated in .bashrc
- university recommends that `module load` commands do not go in .bashrc, rather just put them in the submitted bash script

Slurm:
- `squeue --me` shows queued jobs in pending state
- `sbalance` shows account balance (I think this is shared across the lab)
- each core on Zaratan is 4GB mem
- `#SBATCH --ntasks=<n>` specifies the number of parallel tasks (processes) to launch - you would use this if running multiple instances of your program (so this would be good for me)
- `#SBATCH --cpus-per-task=<n>` is the number of cpu cores assigned to each task. Use this for multithreaded programs (like numpy) that need multiple cores per process

Getting my conda env set up:
- I have project dependencies - how do I know if these are installed on zaratan and how can I install things that are needed? I think that's the point of the conda env - I install in there in my active conda env
- I'm pretty sure that I ended up needing to pip install a few more things - how can I check curiosity to see what I have?
- am I able to change branches when I run this stuff?
-  
- can I pip install my project? I think I will have to pull the git repo into my scratch directory? do I pip install my project in my conda env?
- steps I think I will take:
	- git clone
	- pip install project ?
	- I'm also going to need to install mldsml and switch to the neuralODE-dev branch
	- is it necessary to pip install things from the pyproject.toml when it may be taken care of by just pip installing the project?
	- am I actually able to use wandb to pull datasets down from hpc? wandb on zaratan isn't communicating online but I imagine it can still pull it?
- how do I check the available memory in my scratch directory? I think I might have to store datasets on zaratan directly
- I'm going to have to pull files down from wandb. Is there a way I can pull them down and save them off locally so that we can use datasets that are already created?
- there are data transfer nodes on zaratan - how do I access them?

I am training a neuralODE. I have a git repo with many files, folders, etc. I have config.yaml files that are read in to the main training script, and these config files contain different parameters (like model size, loss function, etc). I want to run many different instances of the training script with different parameters based on these config files, so I am trying to get the code set up to run on my university's HPC. I am creating a conda environmnt to install all required dependencies. I also use another git repository central to my lab which I will have to pull down and pip install. I am looking for advice on how to get everything up and running. A few questions:
- can I pip install the repositories into the conda environment? will the dependencies also be installed?
- am I able to switch branches when I am running things on HPC? I need to be on a different branch of my lab repository
- will pip installing these projects automatically install the dependencies?

do NOT run jobs out of home directory
home directory is the only place where files are backed up - scratch and SHELL are not

```
source ~/scratch/miniforge3/etc/profile.d/conda.sh

conda activate env_name

```
# October 21

I am training neural ODEs to learn 2BP dynamics, and for simplicity I am beginning with the planar case. Through my research, I found that it is very important to each orbit in the training dataset into smaller trajectories because otherwise we see vanishing gradients (hypothesized that when we use the adjoint sensitivity method to compute the gradients, the integrand is overly stable such that when we integrate over time horizons that are too long, we get vanishing gradients). We still think length strategy is important, which refers to training on a portion of the time series of the data to warm start the model and decrease computation time, but in this plot we are just training on the entire time series. 

In this heat map, we are showing the effects of training on different numbers of randomly generated orbits and the effects of segmenting those training orbits into different numbers of segments. When we generate random training data, we set a range for each orbital element and then randomly select from there so our orbits range from LEO to GEO, circular to as eccentric as possible without intersecting the earth. The segmentation ratio indicates what percent of the total trajectory each segment makes up (e.g. 5% means that we segmented the orbit into 20 trajectories, each being 5% of the total orbit). We then test the model on datasets containing different numbers of randomly generated orbital initial conditions, apply the model to the random initial conditions, and find the percent error in the true vs model predicted acceleration averaged over all timesteps and orbits in a given test dataset.

The segmentation trends are generally as expected - the smaller the segment and shorter the time horizon for integration, the lower the error when applying the model to a test dataset. However, we don't see the expected monotonic trend that increasing number of training orbits decreases model error. 

For now, we are focusing on getting the expected smooth gradient in the bottom 3x3 (or 4x4 if necessary) grid of the heat map. Some ideas we've had:
1) to trouble shoot, try to simplify by first testing on ciruclar orbits with different sma. However, this might not be a good idea because when we learn 2BP dynamics we are basically learning the 1/r^2 term, so circular might actually be harder to learn? for context, this is our input layer:
``` python

def sph_4D_rinv_vel(y):
    pos = y[:3]
    radius = jnp.sqrt(jnp.sum(pos**2))
    return jnp.concatenate(
        [
            jnp.array(
                [
                    1 / radius,
                    pos[0] / radius,
                    pos[1] / radius,
                    pos[2] / radius,
                    *y[3:],
                ],
            ),
        ],
    )
```

and this is our output layer:
``` python
def mlp_4D(mlp_output, state, scalar=1.0):
    r_mag = mlp_output[0:1]
    r_dir = mlp_output[1:4]
    acc_pred = r_mag * r_dir
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
2) should we try curriculum training with segmentation strategy? we get better results when we start with small segments, but the loss curves are quite noisy (we should also test different learning rate). maybe train first on a smaller segmentation strategy and then increase it?
3) are our input and output layers sufficient?
4) are the other parameters sufficient?
5) We currently start with a random true anomaly at the first timestep of each orbit (before segmentation) - is this necessary?
---

Let's test on a few different datasets:
**sensitivity-2D-baseline-v1:**
This tests on randomly generated orbits with the following possible ranges/params:
- SMA = \[R_Earth + 200km, R_GEO\] (where GEO is 42164 km)
- e = \[0, 0.99\] (in reality we check for intersections, so the max e is the largest eccentricity without intersecting the earth)
- planar (i = 0, RAAN and argument of periapsis undefined)
![[orbital_elements_hist_baseline 1.png]]
![[multi_dataset_xy_baseline 1.pdf]]

![[heatmap_data_sensitivity_baseline.png]]

![[W&B Chart 10_21_2025, 11_13_27 AM.png]]

![[W&B Chart 10_21_2025, 11_14_09 AM.png]]
**sensitivity-2D-fixednu0-v1**
let's generate datasets using the same random initial conditions but now nu0 = 0 for every dataset. naming scheme will be complex_TBP_planar_fixnu0_{num_orbits}

![[multi_dataset_xy_fixnu0 1.pdf]]
![[orbital_elements_hist_fixnu0.png]]
Now let us train the model
Testing on the baseline datasets with not fixed nu0:

![[heatmap_data_sensitivity_fixnu0.png]]

Testing on fixed nu0:
![[heatmap_data_sensitivity_fixnu0_testfixnu0.png]]


![[W&B Chart 10_21_2025, 11_12_54 AM.png]]
![[W&B Chart 10_21_2025, 11_12_25 AM.png]]

**sensitivity-2D-circular-v1**
This tests on circular orbits with random SMA:
![[multi_dataset_xy_circular.pdf]]

![[orbital_elements_hist_circular.png]]
Now let's train:
![[W&B Chart 10_21_2025, 11_32_15 AM.png]]

![[W&B Chart 10_21_2025, 11_32_57 AM.png]]

Testing on original datasets:
![[heatmap_data_sensitivity_circular_testbaseline.png]]
Testing on circular (training) datasets:
![[heatmap_data_sensitivity_circular.png]]























































# October 16
## Recap
We are working on 2D sensitivity analysis. Preliminary results were surprising - we were expecting to see a monotonic trend that as the number of training orbits increases, the error decreases and that as the segment ratio decreases, the error decreases. The latter is true but the former is not:
![[Pasted image 20251016122314.png]]
![[Pasted image 20251016122305.png]]
We are trying to figure out why this is happening, and want to focus on getting a nice gradient while just looking at the lower left 3x3 or 4x4 grid. 
We also noticed that the loss curves associated with this training are very noisy:
![[Pasted image 20251016122832.png]]
![[Pasted image 20251016122946.png]]
This is probably due to a variety of parameters and hyperparameters. Learning rate may be too high, segmentation strategy may put us in a local minimum that we can't get out of (maybe? double check notes from meeting), might need a training curriculum, etc. Try to get the bottom 3x3 or 4x4 of the heat map behaving expected by testing the interactions of these parameters, implementing training curriculum, etc.

## Sensitivity Analysis
Getting to the bottom of the issue described above...
The issue that we are currently investigating is that as we increase the number of training orbits, our results are not monotonically improving.

### Training data
Let's look at what our training datasets were for the first 4x4 grid:

**TBP_complex_planar_1:**
\[a, e, i, omega, w, nu\] = \[3.5359730e+04 3.7370604e-01 0.0000000e+00           nan           nan
 5.6238999e+00]\
 SMA = 35359.73 km (GEO is ~42 km)
 e = .3737
 nu0 = 322.23 deg
 ![[Pasted image 20251017124525.png|200]]
 
TBP_complex_planar_4:
![[Pasted image 20251017125450.png]]
Note to self - verify that I apply models to the test datasets
 
# October 7
## Recap
- I've begun work on 2D sensitivity studies but need to get more experiments and visualizations up and running. We need to start using the "num_trajs" parameter again for analysis.
- Finished the ASTRA poster yesterday. I made some modifications to the 2BP vector field visualization and experiment that should be documented. 
- I'm having some memory issues when I work locally, but I am having some issues with Docker and installing packages using `pip install -e .`
- I noticed that change $\nu_0$ for the test orbit after training 3D model changes the results largely, which is a bit perplexing since we also segment data. I want to investigate this. 

## Goals
- [x] Fix Docker issues
- [ ] Clean and document vis and experiment from yesterday
- [ ] 2D sensitivity studies. At a min, study num_traj vs segment_length
- [x] quick 3D study so we can talk about it in meeting
- [ ] there is one more stash from the conference I have to go through and apply/document
- [ ] prevent wandb files, artifacts, etc. from being saved locally
## Sensitivity studies
Step 1 is being able to access the parameters we want to study! For now, these include:
- [ ] number of training orbits
- [ ] segmentation strategy
- [ ] length strategy
Segmentation strategy and length strategy are already easily accessible. We also previously utilized `num_trajs` param, but this has basically been phased out in favor of training using the explicit names of datasets defined in the config. So, we can either consistently name datasets and put the num_trajs as a variable within the name (e.g. "simple_TBP_planar_5"), we can dynamically find the number of trajectories by loading the data, we can include another num_trajs param to weep over (but we would have to make sure that we have it properly corresponding to the correct dataset), or we can add a parameter to the dataset itself indicating the number of trajectories. The last option is probably our best bet since reloading data unnecessarily is expensive. 

Basically, this just means that when we train the model we should record how many orbits the model was trained on (taking into account train_test_split). We already have to load the data for training of course, so at that point we can record the number of training orbits to wandb.

I updated the training script and model save function so that we can pass optional additional arguments to log to the model. Currently, I added segment_ratio so we have direct access to the percentage each segment is of the total trajectory, as well as num_total_orbits and num_train_orbits so we don't have to do any additional data manipulation post model training.

Let's run a quick train script and make sure everything is working as expected.

Note - training is taking a very long time to run locally through docker because a lot of space isbeing used on saving artifacts, models, etc. We don't want to do this since everything is just uploaded to wandb and we can pull it down later. 



# October 6
## Recap
Working on sensitivity studies for 2D scenarios (segmentation, amount of training day, length strategy in particular)
## Goals
- [x] Finish ASTRA poster
	- [x] vector field diff plot overlaid with training orbits for a quick glance of how the training data amount impacts training
- [x] Figure out why 3D training isn't working well on test orbit (kinda done)
## 3D training
Last week, I began training on 3D orbits (as opposed to the planar ones we used for the conference). The segment length is 10, i.e. the trajectory is split into 36 segments before training. The loss curves are converging to acceptable values:
![[Pasted image 20251006110049.png]]
I did a sanity chck by testing on a random orbit that fell within the bounds of the training data and got very bad results:
![[Pasted image 20251006110119.png]]
During group meeting, Kruti asked if I had tried differ initial conditions. I had tried altering the shape of the orbit and got similarly bad results, but I had not altered the initial true anomaly.  When I change nu0 from 0 to 20 degrees, I get much better results:
![[Pasted image 20251006110526.png]]
![[Pasted image 20251006110545.png]]


What does this indicate? When we generate data, we take a random initial true anomaly from 0 to 360 degrees. The other orbital elements are randomized according to our desired dataset. When we load the data in, we segment the trajectory. Because of this segmentation, I am surprised that the initial true anomaly would have such a large impact since the model is functionally exposed to *many* different initial true anomalies through segmentation. 
This leads me to wonder: 
1) What does the full predicted vector field look like? How does this change as a function of randomizing vs not randomizing the initial true anomaly (e.g. will results be significantly different if we just always have an initial true anomaly = 0)? If we keep nu0 of the full trajectory randomized, how does the amount of training data affect the model accuracy? We would expect that increased exposure to more ICs, i.e. more training data, would improve accuracy.
2) How important is segmentation in 3D? We expect that it will have a larger effect than in 2D due to the increased dimensionality of the problem - to what extent is this true?

experiment idea for 2d: plot the residual of the vector field against the training orbits

## ASTRA Poster
I want to create the following visualization:
Train on 10 random 2D orbits (complex case). Use all orbits for training, i.e. train/val split = 1. Plot a vector field with the residual of the true versus predicted acceleration, overlayed on top of the training orbits.
- [ ] Generate training data
- [ ] Visualize
- [ ] Update yaml
- [ ] Update train script (use correct yaml)
- [ ] Train
- [ ] Visualize


