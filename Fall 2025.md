# October 21
Working on 2D sensitivity analysis
TBP_complex_1:
 ![[Pasted image 20251017124525.png|200]]
TBP_complex_4:

TBP_complex_16:

TBP_complex_64:

I am training neural ODEs to learn 2BP dynamics, and for simplicity I am beginning with the planar case. Through my research, I found that it is very important to each orbit in the training dataset into smaller trajectories because otherwise we see vanishing gradients (hypothesized that when we use the adjoint sensitivity method to compute the gradients, the integrand is overly stable such that when we integrate over time horizons that are too long, we get vanishing gradients). We still think length strategy is important, which refers to training on a portion of the time series of the data to warm start the model and decrease computation time, but in this plot we are just training on the entire time series. 

In this heat map, we are showing the effects of training on different numbers of randomly generated orbits and the effects of segmenting those training orbits into different numbers of segments. When we generate random training data, we set a range for each orbital element and then randomly select from there so our orbits range from LEO to GEO, circular to as eccentric as possible without intersecting the earth. The segmentation ratio indicates what percent of the total trajectory each segment makes up (e.g. 5% means that we segmented the orbit into 20 trajectories, each being 5% of the total orbit). We then test the model on datasets containing different numbers of randomly generated orbital initial conditions, apply the model to the random initial conditions, and find the percent error in the true vs model predicted acceleration averaged over all timesteps and orbits in a given test dataset.

The segmentation trends are generally as expected - the smaller the segment and shorter the time horizon for integration, the lower the error when applying the model to a test dataset. However, we don't see the expected monotonic trend that increasing number of training orbits decreases model error. 

For now, we are focusing on getting the expected smooth gradient in the bottom 3x3 (or 4x4 if necessary) grid of the heat map. Some ideas we've had:
1) to trouble shoot, try to simplify by first testing on ciruclar orbits with different sma. However, this might not be a good idea because when we learn 2BP dynamics we are basically learning the 1/r^2 term, so circular might actually be harder to learn? for context, this is our input layer:
``` python

def sph_4D_rinv_vel(y):
    pos = y[:3]
    radius = jnp.sqrt(jnp.sum(pos**2))
    return jnp.concatenate(
        [
            jnp.array(
                [
                    1 / radius,
                    pos[0] / radius,
                    pos[1] / radius,
                    pos[2] / radius,
                    *y[3:],
                ],
            ),
        ],
    )
```

and this is our output layer:
``` python
def mlp_4D(mlp_output, state, scalar=1.0):
    r_mag = mlp_output[0:1]
    r_dir = mlp_output[1:4]
    acc_pred = r_mag * r_dir
    return jnp.concatenate((state[3:6], acc_pred), axis=0)
    ```
2) should we try curriculum training with segmentation strategy? we get better results when we start with small segments, but the loss curves are quite noisy (we should also test different learning rate). maybe train first on a smaller segmentation strategy and then increase it?
3) are our input and output layers sufficient?
4) are the other parameters sufficient?
5) We currently start with a random true anomaly at the first timestep of each orbit (before segmentation) - is this necessary?
---

Let's test on a few different datasets:
**sensitivity-2D-baseline-v1:**
This tests on randomly generated orbits with the following possible ranges/params:
- SMA = \[R_Earth + 200km, R_GEO\] (where GEO is 42164 km)
- e = \[0, 0.99\] (in reality we check for intersections, so the max e is the largest eccentricity without intersecting the earth)
- planar (i = 0, RAAN and argument of periapsis undefined)
![[orbital_elements_hist_baseline 1.png]]
![[multi_dataset_xy_baseline 1.pdf]]
![[heatmap_data_sensitivity_baseline.png]]

**sensitivity-2D-fixednu0-v1**
let's generate datasets using the same random initial conditions but now nu0 = 0 for every dataset. naming scheme will be complex_TBP_planar_fixnu0_{num_orbits}
![[multi_dataset_xy_fixnu0.pdf]]
![[orbital_elements_hist_fixnu0.png]]
Now let us train the model
# October 16
## Recap
We are working on 2D sensitivity analysis. Preliminary results were surprising - we were expecting to see a monotonic trend that as the number of training orbits increases, the error decreases and that as the segment ratio decreases, the error decreases. The latter is true but the former is not:
![[Pasted image 20251016122314.png]]
![[Pasted image 20251016122305.png]]
We are trying to figure out why this is happening, and want to focus on getting a nice gradient while just looking at the lower left 3x3 or 4x4 grid. 
We also noticed that the loss curves associated with this training are very noisy:
![[Pasted image 20251016122832.png]]
![[Pasted image 20251016122946.png]]
This is probably due to a variety of parameters and hyperparameters. Learning rate may be too high, segmentation strategy may put us in a local minimum that we can't get out of (maybe? double check notes from meeting), might need a training curriculum, etc. Try to get the bottom 3x3 or 4x4 of the heat map behaving expected by testing the interactions of these parameters, implementing training curriculum, etc.

## Sensitivity Analysis
Getting to the bottom of the issue described above...
The issue that we are currently investigating is that as we increase the number of training orbits, our results are not monotonically improving.

### Training data
Let's look at what our training datasets were for the first 4x4 grid:

**TBP_complex_planar_1:**
\[a, e, i, omega, w, nu\] = \[3.5359730e+04 3.7370604e-01 0.0000000e+00           nan           nan
 5.6238999e+00]\
 SMA = 35359.73 km (GEO is ~42 km)
 e = .3737
 nu0 = 322.23 deg
 ![[Pasted image 20251017124525.png|200]]
 
TBP_complex_planar_4:
![[Pasted image 20251017125450.png]]
Note to self - verify that I apply models to the test datasets
 
# October 7
## Recap
- I've begun work on 2D sensitivity studies but need to get more experiments and visualizations up and running. We need to start using the "num_trajs" parameter again for analysis.
- Finished the ASTRA poster yesterday. I made some modifications to the 2BP vector field visualization and experiment that should be documented. 
- I'm having some memory issues when I work locally, but I am having some issues with Docker and installing packages using `pip install -e .`
- I noticed that change $\nu_0$ for the test orbit after training 3D model changes the results largely, which is a bit perplexing since we also segment data. I want to investigate this. 

## Goals
- [x] Fix Docker issues
- [ ] Clean and document vis and experiment from yesterday
- [ ] 2D sensitivity studies. At a min, study num_traj vs segment_length
- [x] quick 3D study so we can talk about it in meeting
- [ ] there is one more stash from the conference I have to go through and apply/document
- [ ] prevent wandb files, artifacts, etc. from being saved locally
## Sensitivity studies
Step 1 is being able to access the parameters we want to study! For now, these include:
- [ ] number of training orbits
- [ ] segmentation strategy
- [ ] length strategy
Segmentation strategy and length strategy are already easily accessible. We also previously utilized `num_trajs` param, but this has basically been phased out in favor of training using the explicit names of datasets defined in the config. So, we can either consistently name datasets and put the num_trajs as a variable within the name (e.g. "simple_TBP_planar_5"), we can dynamically find the number of trajectories by loading the data, we can include another num_trajs param to weep over (but we would have to make sure that we have it properly corresponding to the correct dataset), or we can add a parameter to the dataset itself indicating the number of trajectories. The last option is probably our best bet since reloading data unnecessarily is expensive. 

Basically, this just means that when we train the model we should record how many orbits the model was trained on (taking into account train_test_split). We already have to load the data for training of course, so at that point we can record the number of training orbits to wandb.

I updated the training script and model save function so that we can pass optional additional arguments to log to the model. Currently, I added segment_ratio so we have direct access to the percentage each segment is of the total trajectory, as well as num_total_orbits and num_train_orbits so we don't have to do any additional data manipulation post model training.

Let's run a quick train script and make sure everything is working as expected.

Note - training is taking a very long time to run locally through docker because a lot of space isbeing used on saving artifacts, models, etc. We don't want to do this since everything is just uploaded to wandb and we can pull it down later. 



# October 6
## Recap
Working on sensitivity studies for 2D scenarios (segmentation, amount of training day, length strategy in particular)
## Goals
- [x] Finish ASTRA poster
	- [x] vector field diff plot overlaid with training orbits for a quick glance of how the training data amount impacts training
- [x] Figure out why 3D training isn't working well on test orbit (kinda done)
## 3D training
Last week, I began training on 3D orbits (as opposed to the planar ones we used for the conference). The segment length is 10, i.e. the trajectory is split into 36 segments before training. The loss curves are converging to acceptable values:
![[Pasted image 20251006110049.png]]
I did a sanity chck by testing on a random orbit that fell within the bounds of the training data and got very bad results:
![[Pasted image 20251006110119.png]]
During group meeting, Kruti asked if I had tried differ initial conditions. I had tried altering the shape of the orbit and got similarly bad results, but I had not altered the initial true anomaly.  When I change nu0 from 0 to 20 degrees, I get much better results:
![[Pasted image 20251006110526.png]]
![[Pasted image 20251006110545.png]]


What does this indicate? When we generate data, we take a random initial true anomaly from 0 to 360 degrees. The other orbital elements are randomized according to our desired dataset. When we load the data in, we segment the trajectory. Because of this segmentation, I am surprised that the initial true anomaly would have such a large impact since the model is functionally exposed to *many* different initial true anomalies through segmentation. 
This leads me to wonder: 
1) What does the full predicted vector field look like? How does this change as a function of randomizing vs not randomizing the initial true anomaly (e.g. will results be significantly different if we just always have an initial true anomaly = 0)? If we keep nu0 of the full trajectory randomized, how does the amount of training data affect the model accuracy? We would expect that increased exposure to more ICs, i.e. more training data, would improve accuracy.
2) How important is segmentation in 3D? We expect that it will have a larger effect than in 2D due to the increased dimensionality of the problem - to what extent is this true?

experiment idea for 2d: plot the residual of the vector field against the training orbits

## ASTRA Poster
I want to create the following visualization:
Train on 10 random 2D orbits (complex case). Use all orbits for training, i.e. train/val split = 1. Plot a vector field with the residual of the true versus predicted acceleration, overlayed on top of the training orbits.
- [ ] Generate training data
- [ ] Visualize
- [ ] Update yaml
- [ ] Update train script (use correct yaml)
- [ ] Train
- [ ] Visualize


