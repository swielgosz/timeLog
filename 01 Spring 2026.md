# February 3
## The Vanishing Gradient Problem for Stiff Neural Differential Equations
## Opt-disc vs disc-opt
Diffrax uses a discretize then optimize method. It differentiates through the discretized solver's operations rather than deriving continuous adjoint ODEs. The default for `diffeqsolve` in Diffrax is `RecursiveCheckpointAdjoint` which means the gradients are computed by differentiating the solver's computation graph (the discrete steps) rather than solving a continuous adjoint system. The original Neural ODE paper uses an optimize-then-discretize approach, the continuous adjoint method. In this method, derive the continuous adjoint equations then discretize them for numerical solution. - Diffrax opts instead to rely on autodiff through the solver’s time-stepping, which discretizes first and then differentiates that discrete computation. 
https://docs.kidger.site/diffrax/api/adjoints/

Stifness in neuralODEs usually comes from the network, not the physics. 

Standard explicit integrators like Runga-Kutta (what we use) are impractical for stiff problems. Explicit integrators compute the next step directly from known information, while implicit integrators define the next state implicitly and it must be solved for at each set. Take $\dot{x}(t) = f(x(t), t)$. 
Explicit step (forward Euler):
$x_{n+1} = x_n + h f(x_n, t_n)$
- Uses only x_n
- One function evaluation
- No equations to solve
Implicit step (Backward Euler):
$x_{n+1} = x_n + h f(x_{n+1}, t_{n+1})$
- $x_{n+1}$ appears on both sides
- Requires solving a nonlinear equation
- Often done with Newton iterations + Jacobians
- Stable but expensive - this is why we don't use them for neural ODEs typically

Implicit methods are the standard for integrating stiff ODEs. Recall that neural ODEs can become stiff throughout the training process, even if the original data comes from non-stiff ODEs, due to the nonlinear dynamics produced by the neural network. 

A- and L-stable integration methods. 

How can we tell if our neural ODE is stiff?
- Solver behavior. During training, log the number of function evaluations, average step size, and rejected steps if available. Warning if NFE steadily increases over epochs or spikes late in training, or if the solver takes much smaller steps than expected for smooth physics. 
- Compare explicit and implicit solvers - if performance improves with an implicit solver, stiffness is probably present

| **Observation**                           | **Interpretation**                  |
| ----------------------------------------- | ----------------------------------- |
| Implicit solver takes much larger steps   | Learned vector field is stiff       |
| Explicit solver NFE ≫ implicit            | Stiffness confirmed                 |
| Implicit gradients are larger / healthier | Gradient damping was solver-induced |

- Jacobian spectrum check
	At sampled states x(t), compute:
	$J = \frac{\partial f_\theta}{\partial x}$ 
	Then look at:
		•	spectral radius
		•	real parts of eigenvalues
	Heuristic
		•	Large negative real eigenvalues (≫ 1 / timestep scale) → stiffness
		•	Rapid variation of eigenvalues along trajectory → stiffness
	Don't need to do this everywhere. Spot checks help. 
- Sensitivity decay
	Check:
	- adjoint norm as a function of time
	- gradient magnitude w.r.t. early-time states
	Classic stiff-Neural-ODE signature:
	- gradients near final time are OK
	- gradients near initial time are ~0
	- worsens as training progresses
	This happens even if loss decreases.
- Perturbation test
	Take a trained model, slightly perturb the state $x \to x + \epsilon$ and integrate forward. Dynamics are stiff if trajectories diverge violently, solver shrinks timestep immediately, or tiny perturbations cause large accelerations. 

Suggestions to help mitigate this:
- Regularize the Jacobian
	- spectral norm penalty
	- Frobenius norm penalty
	- Lipschitz control
- Learn residual dynamics only (UDE)
- Try an IMEX or semi-implicit split 
	- known physics: explicit
	- learned correction: implicit (or damped)
	- ChatGPT says this is very effective
- Don't trust Tsit45 alone - it can hide stiffness 

Let's check if our model is stiff. We will first try using diffrax.ImplicitEuler as our solver and compare it to the results of diffrax.Tsit45. Run id 97oabg96.  We normalized by MEO. 

| label    | wandb_source_id | output_layer         | complex_TBP_nonplanar_1_test (mean) | complex_TBP_nonplanar_1_test (theta-deg) | complex_TBP_nonplanar_1_test (radial%) | complex_TBP_nonplanar_1_test (skewness) | complex_TBP_nonplanar_1_test (pearson-sk1) | complex_TBP_nonplanar_1_test (pearson-sk2) | complex_TBP_nonplanar_1_test (bowley) | complex_TBP_nonplanar_10_test (mean) | complex_TBP_nonplanar_10_test (theta-deg) | complex_TBP_nonplanar_10_test (radial%) | complex_TBP_nonplanar_10_test (skewness) | complex_TBP_nonplanar_10_test (pearson-sk1) | complex_TBP_nonplanar_10_test (pearson-sk2) | complex_TBP_nonplanar_10_test (bowley) | complex_TBP_nonplanar_100_test (mean) | complex_TBP_nonplanar_100_test (theta-deg) | complex_TBP_nonplanar_100_test (radial%) | complex_TBP_nonplanar_100_test (skewness) | complex_TBP_nonplanar_100_test (pearson-sk1) | complex_TBP_nonplanar_100_test (pearson-sk2) | complex_TBP_nonplanar_100_test (bowley) |
| :------- | :-------------- | :------------------- | ----------------------------------: | ---------------------------------------: | -------------------------------------: | --------------------------------------: | -----------------------------------------: | -----------------------------------------: | ------------------------------------: | -----------------------------------: | ----------------------------------------: | --------------------------------------: | ---------------------------------------: | ------------------------------------------: | ------------------------------------------: | -------------------------------------: | ------------------------------------: | -----------------------------------------: | ---------------------------------------: | ----------------------------------------: | -------------------------------------------: | -------------------------------------------: | --------------------------------------: |
| 97oabg96 | 97oabg96        | mlp_4D_unit_softplus |                                 113 |                                     37.4 |                                      0 |                                   0.742 |                                      0.429 |                                      0.707 |                                  0.25 |                                 74.3 |                                      28.9 |                                   0.444 |                                     1.03 |                                       0.187 |                                       0.491 |                                 0.0947 |                                  64.3 |                                       26.8 |                                    0.436 |                                       1.1 |                                        0.112 |                                        0.431 |                                  0.0483 |
Results don't look significantly different, so there is not an indication of stiffness here.

Diffrax does not have a built in way to access the number of function evaluations. What we have is num_steps, num_accepted_steps, and num_rejected_steps. When we are using an adaptive solver like Tsit5, the solver proposes a step size dt, estimates local truncation error, compares error to tolerance, and if error <= tol we accept the step, otherwise reject step and retry with a smaller dt. After a step is accepted, the solver advanced time successfully. We see many rejected steps if dynamics are stiff. 

Why do we take so few steps? Adaptive solvers take steps based on how long the integration interval is ($t_1 - t_0$), how smooth the vector field is, and how loose/tight tolerances are. If $t_1 - t_0$ is small (which it is when we segment our trajectory) and the learned dynamics are smooth, Tsit5 + PID will take large steps and finish in 1-3 accepted steps. Low step count means the solver thinks the ODE is easy over a short interval, and the learned vector field is smooth. Low step counts are bad if you are expecting high-frequency physics or if the adaptive solver is masking model error - training looks good, rollouts look bad, solver steps are very large. This is a known failure mode for neural ODEs - adaptive solvers can hide sharp errors by shrinking or growing dt. 

Things we can check:
- tighten tolerances - if steps jump from ~2 to ~20-50, then the solver is being lenient and is just integrating coarsely. If it stays low, the dynamics are very smooth and maybe too smooth to capture physics
- Fixed step RK4 ablation
```
solver = dfx.RK4()
dt = (t1 - t0) / 10
```
If performance degrades, adaptive solver was doing work and low step count wasn't hiding issues. If performance is similar, learned dynamics are simple and bottleneck is model expressivity/inductive bias, not solver. This suggests > Your performance ceiling is coming from **model structure or learning objective**, not stiffness or solver pathologies. But also very small avg_dt relative to the time scale of your dynamics indicates stiffness. When we normalize time and divide into 90 segments, each segment spans about 0.0698 normalized time. Our average dt is about 0.02 with about 2.75 solver steps, which gives us .055 normalized time which is almost consisten with the 0.0698 figure. 

## Length strategy vs segmentation
## Testing Normalization
Training with l_char = R_MEO: ae7k6gmh

| label    | wandb_source_id | output_layer         | complex_TBP_nonplanar_1_test (mean) | complex_TBP_nonplanar_1_test (theta-deg) | complex_TBP_nonplanar_1_test (radial%) | complex_TBP_nonplanar_1_test (skewness) | complex_TBP_nonplanar_1_test (pearson-sk1) | complex_TBP_nonplanar_1_test (pearson-sk2) | complex_TBP_nonplanar_1_test (bowley) | complex_TBP_nonplanar_10_test (mean) | complex_TBP_nonplanar_10_test (theta-deg) | complex_TBP_nonplanar_10_test (radial%) | complex_TBP_nonplanar_10_test (skewness) | complex_TBP_nonplanar_10_test (pearson-sk1) | complex_TBP_nonplanar_10_test (pearson-sk2) | complex_TBP_nonplanar_10_test (bowley) | complex_TBP_nonplanar_100_test (mean) | complex_TBP_nonplanar_100_test (theta-deg) | complex_TBP_nonplanar_100_test (radial%) | complex_TBP_nonplanar_100_test (skewness) | complex_TBP_nonplanar_100_test (pearson-sk1) | complex_TBP_nonplanar_100_test (pearson-sk2) | complex_TBP_nonplanar_100_test (bowley) |
| :------- | :-------------- | :------------------- | ----------------------------------: | ---------------------------------------: | -------------------------------------: | --------------------------------------: | -----------------------------------------: | -----------------------------------------: | ------------------------------------: | -----------------------------------: | ----------------------------------------: | --------------------------------------: | ---------------------------------------: | ------------------------------------------: | ------------------------------------------: | -------------------------------------: | ------------------------------------: | -----------------------------------------: | ---------------------------------------: | ----------------------------------------: | -------------------------------------------: | -------------------------------------------: | --------------------------------------: |
| ae7k6gmh | ae7k6gmh        | mlp_4D_unit_softplus |                                96.6 |                                     37.2 |                                      0 |                                 -0.0434 |                                    0.00264 |                                     0.0127 |                               -0.0562 |                                 66.7 |                                      28.6 |                                   0.361 |                                    0.651 |                                       0.531 |                                       0.504 |                                  0.165 |                                  57.7 |                                       26.6 |                                    0.375 |                                      1.05 |                                         0.77 |                                        0.555 |                                   0.148 |

Training with l_char = R_Earth: y9i8edcp![[Pasted image 20260203143543.png]]

| label    | wandb_source_id | output_layer         | complex_TBP_nonplanar_1_test (mean) | complex_TBP_nonplanar_1_test (theta-deg) | complex_TBP_nonplanar_1_test (radial%) | complex_TBP_nonplanar_1_test (skewness) | complex_TBP_nonplanar_1_test (pearson-sk1) | complex_TBP_nonplanar_1_test (pearson-sk2) | complex_TBP_nonplanar_1_test (bowley) | complex_TBP_nonplanar_10_test (mean) | complex_TBP_nonplanar_10_test (theta-deg) | complex_TBP_nonplanar_10_test (radial%) | complex_TBP_nonplanar_10_test (skewness) | complex_TBP_nonplanar_10_test (pearson-sk1) | complex_TBP_nonplanar_10_test (pearson-sk2) | complex_TBP_nonplanar_10_test (bowley) | complex_TBP_nonplanar_100_test (mean) | complex_TBP_nonplanar_100_test (theta-deg) | complex_TBP_nonplanar_100_test (radial%) | complex_TBP_nonplanar_100_test (skewness) | complex_TBP_nonplanar_100_test (pearson-sk1) | complex_TBP_nonplanar_100_test (pearson-sk2) | complex_TBP_nonplanar_100_test (bowley) |
| :------- | :-------------- | :------------------- | ----------------------------------: | ---------------------------------------: | -------------------------------------: | --------------------------------------: | -----------------------------------------: | -----------------------------------------: | ------------------------------------: | -----------------------------------: | ----------------------------------------: | --------------------------------------: | ---------------------------------------: | ------------------------------------------: | ------------------------------------------: | -------------------------------------: | ------------------------------------: | -----------------------------------------: | ---------------------------------------: | ----------------------------------------: | -------------------------------------------: | -------------------------------------------: | --------------------------------------: |
| y9i8edcp | y9i8edcp        | mlp_4D_unit_softplus |                                73.4 |                                       38 |                                      0 |                                 -0.0212 |                                     -0.735 |                                     -0.102 |                               -0.0136 |                                 60.7 |                                      28.5 |                                    1.11 |                                   -0.118 |                                      -0.196 |                                      -0.143 |                                -0.0223 |                                  59.7 |                                       26.3 |                                    0.644 |                                    0.0792 |                                        -0.38 |                                        -0.26 |                                   -0.15 |

Other ideas:
1. Per orbit nondimensionalization, but then we have to store scale factors and the model becomes scale conditioned. I don't like this idea
2. Energy-based normalization (physics aware)
	Works bbest when perturbations are weak, energy drifts slowly, scaling is fixed per trajectory. 
3. Canonical (Hamiltonian) normalization: 
	Normalize using canonical units:
	- choose $\mu$ = 1
	- choose characteristic action / length so Hamiltonian is O(1)
	- often used in celestial mechanics, CR3BP, perturbation theory
	- Preserves symplectic structure and keeps Hamiltonian terms balanced
4. Add log-scaled auxiliary features
	- log(r)
	- log(a)
	- normalized energy
	- This preserves physics, give the nn regime awareness, and reduces contortion of vector field
5. Piecewise normalization - split the data into LEO-, MEO-, and GEO-normalized model

## Loss functions
1. Velocity-acceleration consistency loss (assumption free)
	$\mathcal{L}_{\text{cons}} = \left\| \frac{d\hat{x}}{dt} - f_\theta(\hat{x}) \right\|^2$ 
	This means that your predicted trajectory should obey your own learned vector field. Useful for long-horizon stability and prevents solver-cheating behavior
2.  One-step / multi-step rollout loss (agnostic but powerful)
	Train with a mix of short, medium and long rolluts. Loss:
	$\mathcal{L} = \sum_{k \in \{1,5,20\}} \|x(t+k) - \hat{x}(t+k)\|^2$ 
	This makes no assumptions about physics, just says that good dynamics should compose. Usually helps improve generalization more than adding physics constraints. 
3. Phase-space geometry loss (still physics-agnostic)
	Instead of comparing states directly, compare local geometry including distances between nearby points, angles between velocity vectors, curvature of trajectories. Example:
	$\mathcal{L}_{\text{geom}} = \left\| \|x_i - x_j\| - \|\hat{x}_i - \hat{x}_j\| \right\|$ 
	This enforces shape and flow geometry without enforcing what that geometry should be. 
4. Distributional trajectory loss (agnostic, but expensive)
	Treat trajectories as distributions in phase space and compare via sliced Wassertein distance, MMD, energy distance (statistical, not physical). This is assumption free but very heavy and more useful for generative models than prediction. Overkill for now. 
## Feature layers

To do:
- [ ] visualization with different seeds
- [ ] compare models with different characteristic lengths
- [ ] Read Vanishing Gradient paper
- [ ] Try training with new feature layers and output layers
- [ ] Research different normalization 
- [ ] Research different loss function
- [ ] Should larger architecture give better results?
- [ ] Dig into why length strategy was used in the original paper

--- 
# February 2
I was searching for literature using neuralODEs to see if other users have as much difficulty as I am, and I came across [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/html/2512.20643v1). The paper itself isn't great, but I noticed that they searched hidden layer size 2, 3, 4 and a width of 16, 32, 64, 128 and they found that the best architecture was 64 x 3 which interestingly aligns with my findings most of the time. Their other hyperparameters are tanh activation, adam learning rate 1e-3, adam epochs 200, bfgs epochs 200. They were comparing the ability of neural ODEs and UDEs to forecast 33BP dynamics but they were only learning based on one trajectory. I think these results don't mean much, but have again brought up the question of why would a larger network not perform better? Does this indicate a more fundamental issue with the model?

Note to self - the neural ODE architecture doesn't specifically have an input layer. when we indicate depth in our fonig, this is the number of layers that exist before the output layer.  In Equinox, depth is the number of hidden layers; the output layer is added on top of those, and the input is just the input size (not a layer). So with depth=5, you get 5 hidden layers (all width_size) plus 1 final output layer — 6 Linear layers total. If depth=0, it’s just a single Linear(in_size, out_size). So input layers aren't really a "thing" with neural ODEs: The ODE takes a state y(t) and a vector field f(y,t,θ) is computed by a neural net; the “input” is just the current state (and sometimes time or parameters). So it’s more accurate to say “input dimension of the vector field network” than “input layer.” 


# January
Hurray, a new year of research! First goal (and deadline) of the year is to put out a journal paper by start of Spring semester. A list of tasks that need to be done before then
 - [ ] neuralODEs v 1.0

# January 29
Recap: We're not seeing expected trends in sensitivity studies. We expect to see clear gradients of improvement if we increase number of training orbits and probably if we decrease segment length, if we increase architecture size, etc. We suspect there is still something "wrong" with the model - likely the feature or output layers. Are these issues common in all systems? We want to dig into some more literature on neuralODEs from other fields to see if this issues were present. 

Does it make sense to normalize and then apply feature layer?
To do:
- [ ] test segmentation vs length strat - why did the original authors introduce a length strategy and not just shorten trajectories?
- [ ] why do we not see improving performance with larger architecture? is there a reason or is our model just not correct yet?
- [ ] create central analysis script
- [ ] continue cleaning neuralODE.py
- [ ] fix scaling in eccentricity vs sma
- [ ] put color bars in log scale
- [ ] 
# January 28
- [ ] Create central analysis script

# January 27
By the end of the day, we should have the experiments and visualizations set up and functional. Don't focus on looks (e.g. perfect fontsize and spacing, etc) yet, rather focus on content. Tomorrow, we should be able to compile results. By the end of the day, we should have a skeleton report of figures and tables desired, and we can fill it in tomorrow. 
## Sensitivity Studies
Model sensitivities: feature layers, output layers, feature transforms/normalizations, architecture, physical parameters (?). Pipeline sensitivities: optimization settings, data splits, batch/epoch schedule, augmentation, random seed, **fitting** of normalization stats Model sensitivities

Model sensitivities:
- [ ] feature layers
- [ ] output layers
- [x] architecture
- [ ] training data
	- [ ] number of orbits
	- [ ] complexity
- [ ] segment length


Pipeline:
- [ ] Optimization settings
	- [ ] atol
	- [ ] rtol
	- [ ] learning rate
- [ ] batch schedule
- [ ] seed

- Is length strategy or segmentation strategy more important? Does a combination help? we already know that length strat is useless if the training data isn't additionally segmented, which implies segmentation is more important. However, does using length strategy in addition actually help?
- Is there a significant difference between if do train/test split or test/train split? This may not be relevant because we have separate training and validation datasets. 
- Vector field vi
### Seeding study
For this study, we want to see the effects of training with different random seeds. We want to show boxplots and maybe violin plots to represent how the error metric of interest changes as a function of the seed. 

We ran a sweep of 10 different seeds, which should be sufficient to get some preliminary results. We can partially base the structure of the experiment off of the `IntegrationExperiment.py`. We will need to load a wandb group which contains runs with different seeds. Do we care about future proofing right now? What I mean is, should we assume that each time we load a wandb group it contains only one run per seed, or should we compile results such that if we have multiple runs per seed with each having different hyperparameters, then we average the results for each seed and plot the averaged results? Let's just focus on the former for now. 

So, we need to take a group of runs each with a different seed. In general, we have been training models on training sets and then testing on separate validation datasets. To create error distributions of any sort (box plots, violin plots, etc), we need to calculate one scalar error metric per model. We already have this implemented in some of the work I've already been doing, but we should clean up that code and separate out the experiments and visualizations. Let's find which script I had that was already the most similar to this and let's parse out the important info. 

First important note - all of our error metric calculations are in `metrics.py`. Some changes were made in this since the conference - let's review them and see if the changes make sense still. 

!! In the original metrics.py used in the conference, the AccelerationMetric was actually calculating the average of error in the entire derivative state (i.e. velocity and acceleration, not just acceleration). We fixed this. 

The acceleration error metric class is now set up in a more logical way. We now want to create an experiment where we run through all of the models in a group, calculate the acceleration error for each model applied to a validation dataset, and then use that mean acceleration error to create a violin plot for the error distribution. Maybe also implement for if we have different models (hyperparameters)? Does this actually need an experiment? We could format this similar to IntegrationExperiment and basically just record the mean acceleration per run.

want to create an AccelerationExperiment class similar to IntegrationExperiment, where the goal is to return results related to the acceleartion metrics. This might include the idx, true and predicted acceleration at each time, and the statistics that are contained within AccelerationExperiment. 

Now, I would like to create a visualizer where we create a violin plot to study the distribution of acceleration errors when we run different seeds. This will look like calling runs=query_runs_in_group("sweep-seed-1-26"), looping through each run and  recording the seed number from config.parameters.seed, evaluating the mean acceleration error when applying the model to a test dataset, then creating a violin plot to show the distribution of acceleration error across all seeds. Apply normalization to testing datasets before computing acceleration error like we do in analyze_2BP.py to make sure everything is scaled correctly. let's apply the models to the testing dataset "complex_TBP_planar_10_test". Save the violin plot as a pdf and png. Does this all make sense? Also, it would be beneficial but not necessary if we had a way of evaluating results if there were multiple models with the same seed. I'm not sure if it would make more sense to average results across models with the same seed, or use paired comparisons like you mentioned earlier. Do you think that would be easy/beneficial or we can just do it later if we want to?

Other visualizations to implement:
- feature layers
- output layers
- 
 feature layers, output layers, feature transforms/normalizations, architecture, physical parameters

--- 
# January 26
Goals for the day:
- [ ] Build out skeleton of sensitivity report and complete as much as possible
- [x] Create a copy of current repository
- [ ] Clean current repository

## Cleaning repository
1. I am moving all work done to train latent and UDE codes to the folder `Latent and UDE codes` on Obsidian, and I can reference these later. Same with the data generation codes for the data used to train those models. 
The most cluttering aspect of the code is the training phase visualization. We collect a lot of information within neuralODE.py -- we need a reference trajectory to provide a ground truth orbit for comparison, then we collect information at each phase. of the training. It seems like there should bea simpler way to do this. Basically what we want to do is copy the model at different phases in the training pipeline as desired. 

## Sensitivity studies
We want to explore model vs pipeline sensitivities. Model sensitivities: feature layers, output layers, feature transforms/normalizations, architecture, physical parameters (?). Pipeline sensitivities: optimization settings, data splits, batch/epoch schedule, augmentation, random seed, **fitting** of normalization stats Model sensitivities

- Seeding study
- Model size study

### Model seeding experiment
Note to self - seed is the same for the model itself
# January 20

We have a MWE of a learned perturbation working correctly. Today, we want to extend this to 2BP perturbations at a minimum. If there is time, we also want to learn 4th body effects when going from CR3BP to 4BP.

A lot of the functionality in train_UDE.py is similar to functionality in neuralODE.py. I want to be able to generalize neuralODE.py and/or update train_UDE.py to use functionality that already exists in neuralODE.py. Maybe we make a neuralUDE.py with additional functionality?

Before we do this, let's clean up neuralODE.py some. Functionality that has been added for debugging/analysis can probably be both condensed and some of it moved to a utils file. 
# January 19
## Questions/follow up
- How would training change if we used an implicit solver instead of an explicit solver?

Worked on perturbation code. Notes in Journal Paper section.

# January 14
To do:
- [ ] Reread SALAMANDER
	- [ ] review UDEs - how are these different from neuralODEs?
- [ ] Generate toy dataset with 2BP dynamics and simplest perturbations. Probably SRP - find a paper for this? 
- [ ] Train neuralODE to learn unknown dynamic components
- [ ] Train latentODE to learn unknown dynamics components
	- [ ] Before this, we should introduce an error metric of accumulated position error since we can't directly access the accelerations in meaningful units (confirm that this is true). We should also record accumulated position error for vanilla neural ODE so we have a basis of comparison

I am currently intimidated by starting unknown force stuff because I don't know how to do it but that is the point!!! Let's get an idea of what we're doing, put some words (code) to paper (screen) and then we can iterate on that. Let's build out a pseudocode first. 

## Universal Differential Equations
Neural ODE vs UDE:
A neural ODE learns the entire right-hand side of an ODE using a neural network:
$\dot{x}(t)=f_\theta(x(t),t)$ 
e.g. $f_\theta$ is purely learned. No explicit physics, constraints, or structure are included unless you add it manually; the model discovers everything from data. 
A Universal Differential Equation (UDE) keeps known physics and inserts a neural network only where the model is incomplete or uncertain:
$\dot{x}(t)=f_{\textrm{known}}(x,t)+f_\theta(x,t)$ 
e.g. the neural network acts as a closure term, correction , or unknown force. This is much more data-efficient. 
# January 7
Today, I need to develop an initial narrative for my journal paper and determine what is left to be done. I'm not totally sure what story I am trying to tell, because I haven't gotten to the portion of the research focusing on forced linearization of latent ODEs to find trajectories in cislunar space. It looks like the paper will focus on extending the results of the conference paper but I'm not sure if the "story" itself will develop?

 Working at my parents' house was very difficult, so I need to buckle down extra hard the next few weeks. 
To do:
- [ ] try training on 3D - compare mlp4D_unit to the other variations
- [ ] Determine what sensitivities we want to include in the journal paper
	- [ ] Plan associated visualizations - at least draw them if not actually making them
	- [ ] Plan associated tables
- [ ] Determine what CR3BP orbital families we want to include
	- [ ] Generate data
- [ ] Latent ODE work
	- [ ] Try training with MLP in GRU instead of RNN
	- [ ] Read original paper and take notes
	- [ ] Watch some of the videos that are located in Technical Notes/ Latent ODEs
- [ ] figure out when mlp4D_signed is worse (because intuitively it should be)
- [ ] CLEAN CODE
	- [ ] work on this after hours
	- [ ] a lot of stuff is repetitive and can probably can be condensed 

## Investigating output layer effects
We expect mlp_4D_signed to produce the worst outputs as discused in detail in Fall 2025 notes. It doesn't (at least for the 2D case). Thoughts:
- we need new metrics to test if physics are being obeyed. This would most likely be in the form of observig the acceleration direction - does it flip? how far does it stray from radial? etc
- It may also have more of an effect on training on 3D datasets. Let's investigate this first

### Training on 3D data
At some point we generated 3D datasets, but it was long enough ago that we should regenerate to make sure we're getting what we want. Let's use a naming convention analogous to that of our 2BP planar, which was `<complex|simple>_TBP_planar_<num_orbits>_<train|test>`. 
Let's replace "planar" with "nonplanar", and generate datasets for 1, 10, 100 orbits, simple and complex, train and test to begin with. 
Datasets to generate:
- [x] simple_TBP_nonplanar_1_train
- [x] simple_TBP_nonplanar_1_test
- [x] simple_TBP_nonplanar_10_train
- [x] simple_TBP_nonplanar_10_test
- [x] simple_TBP_nonplanar_100_train
- [x] simple_TBP_nonplanar_100_test
- [x] complex_TBP_nonplanar_1_train
- [x] complex_TBP_nonplanar_1_test
- [x] complex_TBP_nonplanar_10_train
- [x] complex_TBP_nonplanar_10_test
- [x] complex_TBP_nonplanar_100_train
- [x] complex_TBP_nonplanar_100_test

Now let's visualize the training dataset. 
![[Pasted image 20260107200043.png]]
![[Pasted image 20260107200051.png]]
Great, now let's train using mlp_4D_unit and compare to other output layers
Training progress: 100%|██████████| 3000/3000 [09:06<00:00,  5.49step/s] [repeated 25x across cluster]
sweep.py finished in 28.2 minutes (1693.9 seconds) for sweep "complex_TBP_nonplanar_10_v0"

  ![[Pasted image 20260108111913.png]]
  These results look more promising! We no longer see the results of the planar case where mlp4D_signed is typically the best. However, there are more instances of incomplete results for softplus and logmag output layers. Interestingly, these occur only for a segment length of 10 (10/360 = 2.7% of the total orbits), rather than 4 (1.1%) or 18 (20%)

Let's take a look at the loss curves of the most successful runs. These are the three best runs from training on 100 orbits. 
The best from the top right subplot is segment length = 4, output = logmag_unit_exp, no lenght strat. This is run happy-dragon-21070 with runid slulhbac. 

It seems like there is a balance of length strategy and segmentation legnth to strike. If a segment is of length 4 and then we use the length strat, that is essentially useless I think (but let's poke further into this later). We may want to plot lenght strat vs segmentation length.

In general when we put in some actual trajectory examples
When we train on segmented data, we learn the dynamics based on each trajectory. Each trajectory has the same dynamics. When we then recreate an orbit, we no longer 

Standing questions/thoughts:
Does a segmentation curriculum help? 
Does a length strategy curriculum help?
Do the above have an interplaying role?
Learning rate curriculum should always help
Review what John meant by model vs pipeline (I think those were the terms he used?)
Does using other output layers help in the case of planar? Maybe we obey physics better?

# January 8