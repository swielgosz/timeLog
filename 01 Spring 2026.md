# January
Hurray, a new year of research! First goal (and deadline) of the year is to put out a journal paper by start of Spring semester. A list of tasks that need to be done before then
 - [ ] neuralODEs v 1.0

# January 28
Things to finish before meeting:
- [ ] Fix normalization of SMA in the heatmap
- [ ] 
- [ ] 

Final visualizations:
- vector field
- n orbits vs segmentation length heat map
- 
# January 27
By the end of the day, we should have the experiments and visualizations set up and functional. Don't focus on looks (e.g. perfect fontsize and spacing, etc) yet, rather focus on content. Tomorrow, we should be able to compile results. By the end of the day, we should have a skeleton report of figures and tables desired, and we can fill it in tomorrow. 
## Sensitivity Studies
Model sensitivities: feature layers, output layers, feature transforms/normalizations, architecture, physical parameters (?). Pipeline sensitivities: optimization settings, data splits, batch/epoch schedule, augmentation, random seed, **fitting** of normalization stats Model sensitivities

Model sensitivities:
- [ ] feature layers
- [ ] output layers
- [x] architecture
- [ ] training data
	- [ ] number of orbits
	- [ ] complexity
- [ ] segment length


Pipeline:
- [ ] Optimization settings
	- [ ] atol
	- [ ] rtol
	- [ ] learning rate
- [ ] batch schedule
- [ ] seed

- Is length strategy or segmentation strategy more important? Does a combination help? we already know that length strat is useless if the training data isn't additionally segmented, which implies segmentation is more important. However, does using length strategy in addition actually help?
- Is there a significant difference between if do train/test split or test/train split? This may not be relevant because we have separate training and validation datasets. 
- Vector field vi
### Seeding study
For this study, we want to see the effects of training with different random seeds. We want to show boxplots and maybe violin plots to represent how the error metric of interest changes as a function of the seed. 

We ran a sweep of 10 different seeds, which should be sufficient to get some preliminary results. We can partially base the structure of the experiment off of the `IntegrationExperiment.py`. We will need to load a wandb group which contains runs with different seeds. Do we care about future proofing right now? What I mean is, should we assume that each time we load a wandb group it contains only one run per seed, or should we compile results such that if we have multiple runs per seed with each having different hyperparameters, then we average the results for each seed and plot the averaged results? Let's just focus on the former for now. 

So, we need to take a group of runs each with a different seed. In general, we have been training models on training sets and then testing on separate validation datasets. To create error distributions of any sort (box plots, violin plots, etc), we need to calculate one scalar error metric per model. We already have this implemented in some of the work I've already been doing, but we should clean up that code and separate out the experiments and visualizations. Let's find which script I had that was already the most similar to this and let's parse out the important info. 

First important note - all of our error metric calculations are in `metrics.py`. Some changes were made in this since the conference - let's review them and see if the changes make sense still. 

!! In the original metrics.py used in the conference, the AccelerationMetric was actually calculating the average of error in the entire derivative state (i.e. velocity and acceleration, not just acceleration). We fixed this. 

The acceleration error metric class is now set up in a more logical way. We now want to create an experiment where we run through all of the models in a group, calculate the acceleration error for each model applied to a validation dataset, and then use that mean acceleration error to create a violin plot for the error distribution. Maybe also implement for if we have different models (hyperparameters)? Does this actually need an experiment? We could format this similar to IntegrationExperiment and basically just record the mean acceleration per run.

want to create an AccelerationExperiment class similar to IntegrationExperiment, where the goal is to return results related to the acceleartion metrics. This might include the idx, true and predicted acceleration at each time, and the statistics that are contained within AccelerationExperiment. 

Now, I would like to create a visualizer where we create a violin plot to study the distribution of acceleration errors when we run different seeds. This will look like calling runs=query_runs_in_group("sweep-seed-1-26"), looping through each run and  recording the seed number from config.parameters.seed, evaluating the mean acceleration error when applying the model to a test dataset, then creating a violin plot to show the distribution of acceleration error across all seeds. Apply normalization to testing datasets before computing acceleration error like we do in analyze_2BP.py to make sure everything is scaled correctly. let's apply the models to the testing dataset "complex_TBP_planar_10_test". Save the violin plot as a pdf and png. Does this all make sense? Also, it would be beneficial but not necessary if we had a way of evaluating results if there were multiple models with the same seed. I'm not sure if it would make more sense to average results across models with the same seed, or use paired comparisons like you mentioned earlier. Do you think that would be easy/beneficial or we can just do it later if we want to?

Other visualizations to implement:
- feature layers
- output layers
- 
 feature layers, output layers, feature transforms/normalizations, architecture, physical parameters

--- 
# January 26
Goals for the day:
- [ ] Build out skeleton of sensitivity report and complete as much as possible
- [x] Create a copy of current repository
- [ ] Clean current repository

## Cleaning repository
1. I am moving all work done to train latent and UDE codes to the folder `Latent and UDE codes` on Obsidian, and I can reference these later. Same with the data generation codes for the data used to train those models. 
The most cluttering aspect of the code is the training phase visualization. We collect a lot of information within neuralODE.py -- we need a reference trajectory to provide a ground truth orbit for comparison, then we collect information at each phase. of the training. It seems like there should bea simpler way to do this. Basically what we want to do is copy the model at different phases in the training pipeline as desired. 

## Sensitivity studies
We want to explore model vs pipeline sensitivities. Model sensitivities: feature layers, output layers, feature transforms/normalizations, architecture, physical parameters (?). Pipeline sensitivities: optimization settings, data splits, batch/epoch schedule, augmentation, random seed, **fitting** of normalization stats Model sensitivities

- Seeding study
- Model size study

### Model seeding experiment
Note to self - seed is the same for the model itself
# January 20

We have a MWE of a learned perturbation working correctly. Today, we want to extend this to 2BP perturbations at a minimum. If there is time, we also want to learn 4th body effects when going from CR3BP to 4BP.

A lot of the functionality in train_UDE.py is similar to functionality in neuralODE.py. I want to be able to generalize neuralODE.py and/or update train_UDE.py to use functionality that already exists in neuralODE.py. Maybe we make a neuralUDE.py with additional functionality?

Before we do this, let's clean up neuralODE.py some. Functionality that has been added for debugging/analysis can probably be both condensed and some of it moved to a utils file. 
# January 19
## Questions/follow up
- How would training change if we used an implicit solver instead of an explicit solver?

Worked on perturbation code. Notes in Journal Paper section.

# January 14
To do:
- [ ] Reread SALAMANDER
	- [ ] review UDEs - how are these different from neuralODEs?
- [ ] Generate toy dataset with 2BP dynamics and simplest perturbations. Probably SRP - find a paper for this? 
- [ ] Train neuralODE to learn unknown dynamic components
- [ ] Train latentODE to learn unknown dynamics components
	- [ ] Before this, we should introduce an error metric of accumulated position error since we can't directly access the accelerations in meaningful units (confirm that this is true). We should also record accumulated position error for vanilla neural ODE so we have a basis of comparison

I am currently intimidated by starting unknown force stuff because I don't know how to do it but that is the point!!! Let's get an idea of what we're doing, put some words (code) to paper (screen) and then we can iterate on that. Let's build out a pseudocode first. 

## Universal Differential Equations
Neural ODE vs UDE:
A neural ODE learns the entire right-hand side of an ODE using a neural network:
$\dot{x}(t)=f_\theta(x(t),t)$ 
e.g. $f_\theta$ is purely learned. No explicit physics, constraints, or structure are included unless you add it manually; the model discovers everything from data. 
A Universal Differential Equation (UDE) keeps known physics and inserts a neural network only where the model is incomplete or uncertain:
$\dot{x}(t)=f_{\textrm{known}}(x,t)+f_\theta(x,t)$ 
e.g. the neural network acts as a closure term, correction , or unknown force. This is much more data-efficient. 
# January 7
Today, I need to develop an initial narrative for my journal paper and determine what is left to be done. I'm not totally sure what story I am trying to tell, because I haven't gotten to the portion of the research focusing on forced linearization of latent ODEs to find trajectories in cislunar space. It looks like the paper will focus on extending the results of the conference paper but I'm not sure if the "story" itself will develop?

 Working at my parents' house was very difficult, so I need to buckle down extra hard the next few weeks. 
To do:
- [ ] try training on 3D - compare mlp4D_unit to the other variations
- [ ] Determine what sensitivities we want to include in the journal paper
	- [ ] Plan associated visualizations - at least draw them if not actually making them
	- [ ] Plan associated tables
- [ ] Determine what CR3BP orbital families we want to include
	- [ ] Generate data
- [ ] Latent ODE work
	- [ ] Try training with MLP in GRU instead of RNN
	- [ ] Read original paper and take notes
	- [ ] Watch some of the videos that are located in Technical Notes/ Latent ODEs
- [ ] figure out when mlp4D_signed is worse (because intuitively it should be)
- [ ] CLEAN CODE
	- [ ] work on this after hours
	- [ ] a lot of stuff is repetitive and can probably can be condensed 

## Investigating output layer effects
We expect mlp_4D_signed to produce the worst outputs as discused in detail in Fall 2025 notes. It doesn't (at least for the 2D case). Thoughts:
- we need new metrics to test if physics are being obeyed. This would most likely be in the form of observig the acceleration direction - does it flip? how far does it stray from radial? etc
- It may also have more of an effect on training on 3D datasets. Let's investigate this first

### Training on 3D data
At some point we generated 3D datasets, but it was long enough ago that we should regenerate to make sure we're getting what we want. Let's use a naming convention analogous to that of our 2BP planar, which was `<complex|simple>_TBP_planar_<num_orbits>_<train|test>`. 
Let's replace "planar" with "nonplanar", and generate datasets for 1, 10, 100 orbits, simple and complex, train and test to begin with. 
Datasets to generate:
- [x] simple_TBP_nonplanar_1_train
- [x] simple_TBP_nonplanar_1_test
- [x] simple_TBP_nonplanar_10_train
- [x] simple_TBP_nonplanar_10_test
- [x] simple_TBP_nonplanar_100_train
- [x] simple_TBP_nonplanar_100_test
- [x] complex_TBP_nonplanar_1_train
- [x] complex_TBP_nonplanar_1_test
- [x] complex_TBP_nonplanar_10_train
- [x] complex_TBP_nonplanar_10_test
- [x] complex_TBP_nonplanar_100_train
- [x] complex_TBP_nonplanar_100_test

Now let's visualize the training dataset. 
![[Pasted image 20260107200043.png]]
![[Pasted image 20260107200051.png]]
Great, now let's train using mlp_4D_unit and compare to other output layers
Training progress: 100%|██████████| 3000/3000 [09:06<00:00,  5.49step/s] [repeated 25x across cluster]
sweep.py finished in 28.2 minutes (1693.9 seconds) for sweep "complex_TBP_nonplanar_10_v0"

  ![[Pasted image 20260108111913.png]]
  These results look more promising! We no longer see the results of the planar case where mlp4D_signed is typically the best. However, there are more instances of incomplete results for softplus and logmag output layers. Interestingly, these occur only for a segment length of 10 (10/360 = 2.7% of the total orbits), rather than 4 (1.1%) or 18 (20%)

Let's take a look at the loss curves of the most successful runs. These are the three best runs from training on 100 orbits. 
The best from the top right subplot is segment length = 4, output = logmag_unit_exp, no lenght strat. This is run happy-dragon-21070 with runid slulhbac. 

It seems like there is a balance of length strategy and segmentation legnth to strike. If a segment is of length 4 and then we use the length strat, that is essentially useless I think (but let's poke further into this later). We may want to plot lenght strat vs segmentation length.

In general when we put in some actual trajectory examples
When we train on segmented data, we learn the dynamics based on each trajectory. Each trajectory has the same dynamics. When we then recreate an orbit, we no longer 

Standing questions/thoughts:
Does a segmentation curriculum help? 
Does a length strategy curriculum help?
Do the above have an interplaying role?
Learning rate curriculum should always help
Review what John meant by model vs pipeline (I think those were the terms he used?)
Does using other output layers help in the case of planar? Maybe we obey physics better?

# January 8